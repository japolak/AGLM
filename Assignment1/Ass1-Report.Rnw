%% Do not edit preable unless you know what you are doing.
\documentclass{article}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{dsfont}
\usepackage{indentfirst}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{enumitem}

% No indents in whole document
\setlength\parindent{0pt}

% Gaps between item is itemize
\setitemize{itemsep=-1em,topsep=0.5em,parsep=0em,partopsep=0pt}

% Margins of the document
\geometry{verbose,tmargin=2.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}

% Header and footer for all pages
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\rfoot{Page \thepage}

% Header and footer for first page
\fancypagestyle{plain}{%
  \renewcommand{\headrulewidth}{0pt}%
  \fancyhf{}%
  \rhead{ETH Zurich}
  \lhead{Applied Generalized Linear Models \\ Spring Semester 2020}
  \rfoot{Page \thepage}
}

% Global Settings of R-code
<<setup, include=FALSE, cache=FALSE>>=
# Install these packages if you don't have them already
#install.packages("knitr","dplyr","ggplot2","multcomp")
library(dplyr)
library(multcomp)
library(ggplot2)
library(knitr)
# set global chunk options
opts_chunk$set(fig.path='figures/plot-', fig.align='center', fig.show='hold')
options(formatR.arrow=TRUE,width=90)
@

% Mathematics Operators for Ease of Notation
\DeclareMathOperator{\SSR}{SSR}
\DeclareMathOperator{\SSreg}{SSreg}
\DeclareMathOperator{\SST}{SST}

% Bad Box settings
\hbadness=10000
\vbadness=\maxdimen
\vfuzz=30pt
\hfuzz=30pt


%% ---------- BEGIN DOCUMENT -----------------------------------------------
\begin{document}

%% Tiltle
\title{Assignment 1}
\author{Milan Kuzmanovic, Mark McMahon \\ Martin Kotuliak, Jakub Polak}
\date{\today}
\maketitle

\section*{Task 1}

A multiple linear regression model has been estimated to study the relation- ship between $Y =$ violent crime rate (per 100,000 people), $X_1 =$ poverty rate (percentage with income below the poverty line) and $X_2 =$ percentage living in urban area. Data are collected in 51 cities in the U.S.\\

The relevant equations that relate estimate, standard error, T-statistic, R-squared and Sum of Squares of residuals, regression and total are folloqwing.

\[ \frac{\beta_i}{se(\beta_i)} = T_i \qquad R^2 = \frac{\SSreg}{\SST}= 1-\frac{\SSR}{\SST}  \qquad \SST = \SSreg + \SSR
\]

We plug in the corresponding information that is already provided and compute the missing values.

<<t1-table, echo=TRUE, include=TRUE, eval=TRUE, results='hold'>>=
a <- -498.683 / 140.988
b <- 4.885 * 6.677
c <- 9.112 / 6.900
d <- 1841257.15 / (1 - 0.5708)
e <- d - 1841257.15
@

The table below reports the output with filled in missing information.

\begin{center}
\begin{tabular}{ l r r r r }
 \hline
            & Est.            & s.e.              & t-value         & p-value \\
 \hline
 Intercept  & -498.683  & 140.988 & $^{a\;}$\Sexpr{round(a,3)} & 0.009 \\
 $X_1$      & $^{b\;}$\Sexpr{round(b,3)} & 6.677  & 4.885     & 0.001 \\
 $X_2$      & 9.112  & $^{c\;}$\Sexpr{round(c,3)} & 6.900      & 0.001 \\
 \hline
 $R^2$      & 0.5708            & & & \\
 SSreg      & $^{e\;}$\Sexpr{format(e,scientific=FALSE,nsmall=2)} & & & \\
 SSR        & 1841257.15        & & & \\
 SSTotal    & $^{d\;}$\Sexpr{format(d,scientific=FALSE,nsmall=2)} & & & \\
 \hline
\end{tabular}
\end{center}

The the coefficient of determination $R^2$ is $0.5708$. This value measures the proportion of the variance in $Y$ explained by the model. Hence, 57.08 \% of the sample variability of $Y$ can be explained by the linear combination of $X_i$'s given the sample data. \\

To compute the overall F-test we use the equation below and the statistic then follows an F distribution with corresponding degrees of freedom.
\[ F = \frac{\SSreg/p}{\SSR/(n-(p+1))} \sim F_{p,n-(p+1)}
\]

The data are collected in 51 cities, so $n=51$ and we have 2 predictors, so $p=2$. Other values we can easily obtain from the filled table above.
<<t1-statistic, include=TRUE>>=
(f = (e / 2) / (1841257.15 / (51-(2+1)) ))
@

Hence, the F-statistic has a value of \Sexpr{f}. To interpret this, the global F-test, tests a null hypothesis that all regression coefficients are simultaneously 0. In a mathematical notation $H_0 : \beta_1 = \beta_2 = 0$. To evaluate the test, we can compute its p-value. It is a quantile of the corresponding F-distribution for the given statistic or mass under the distribution.

<<t1-pvalue, include=TRUE>>=
(p = pf(f,2,48,lower.tail = FALSE))
@

The p-value is \Sexpr{p}, which is lower than the significance level $\alpha=0.05$ and therefore, there is a significant evidence against the null hypothesis that all regression coefficients are simultaneously zero. Hence, the p-value of a given sample is small and it suggests that the model with all the covariates is better than the one with just intercept coefficient. This test determines the linear model is suitable to explain some of the variance in the outcome variable. Which in this case it does.

\section*{Task 2}

The table below shows the scores of the first test (maximum score 10 points) in a beginning German course. Students in the course are grouped as follows:
\begin{itemize}
\item Group A: Never studied foreign language before, but have good English skills\\
\item Group B: Never studied foreign language before, have poor English skills\\
\item Group C: Studied other foreign language
\end{itemize}

\begin{center}
\begin{tabular}{ c c c }
 \hline
 Group A & Group B & Group C \\
 \hline
 4 & 1 & 9 \\
 6 & 5 & 10 \\
 8 &   & 5 \\
 \hline
\end{tabular}
\end{center}

Two-sample t-test is usual method for comparing mean scores of two groups. The one-way analysis of variance (ANOVA), also known as one-factor ANOVA, is an extension of independent two-samples t-test for comparing means in a situation where there are more than two groups. This corresponds to our situation.\\

The Hypothesis test in ANOVA is following:
\begin{itemize}
\item Null hypothesis: the means of the different groups are the same \\
\item Alternative hypothesis: At least one sample mean is not equal to the others
\end{itemize}

The Assumptions of ANOVA are following:
\begin{itemize}
\item The observations are obtained independently and randomly from the population defined by the groups of factor levels.\\
\item The data of each factor level are normally distributed.\\
\item These normal populations have a common variance.
\end{itemize}

The F-test is used for comparing the factors of the total deviation. In ANOVA, F-statistic is computed as the ratio of the variance between groups and variance within groups. The F-statistic is then compared to the F-distribution with $I-1$ and $n-I$ degrees of freedom, where $I =$ total number of groups and $n =$ total number of observations. Note that, a lower ratio (ratio $< 1$) indicates that there are no significant difference between the means of the samples being compared. However, a higher ratio implies that the variation among group means are significant.\\

The process of ANOVA testing is following:
\begin{itemize}
\item Compute the common variance, which is called variance within samples $S^2_{\text{within}}$ or residual variance.\\
\item Compute the variance between sample means, by first computing the mean of each group and then computing the variance between sample means $S^2_{\text{between}}$. \\
\item Produce F-statistic as the ratio of $F = \frac{S^2_{\text{between}} / (I-1)}{S^2_{\text{within}}/(n-I)} $.
\end{itemize}

From the F-statistic we compute the p-value, ie the probability of obtaining test results at least as extreme as the statistics actually observed during the test, assuming that the null hypothesis is correct.\\

With the code below, we specify the data and compute common statistics for each group.
<<t2-data>>=
X <- data.frame("grade" = c(4,6,8,1,5,9,10,5),
                "group" = c("A","A","A","B","B","C","C","C"))

library(dplyr)
data.frame(group_by(X, group) %>%
  summarise( count = n(), mean = mean(grade), var = var(grade), sd = sd(grade)))
@

These can be easily visualised with a boxplot (left) or a group means plot with errorbars signifying one standard deviation (right). The main observation we conclude from these plot is the large variance within groups caused mainly by having just few observations.

<<t2-plots, echo=FALSE, fig.width=4, fig.height=4, out.width='.4\\linewidth'>>=
boxplot(grade ~ group, data=X, col=2:5, frame = FALSE, xlab = "Groups", ylab="Grade")
points(grade ~ group, data=X,pch=20)

Xplot <- X
out = group_by(Xplot, group) %>%
  summarise( count = n(), mean = mean(grade), var = var(grade), sd = sd(grade))
Xplot$mean <- c(rep(out$mean[1],3),rep(out$mean[2],2),rep(out$mean[3],3))
Xplot$sd <- c(rep(out$sd[1],3),rep(out$sd[2],2),rep(out$sd[3],3))
ggplot(Xplot) +
  geom_errorbar(aes(x= group, ymin= mean-sd, ymax = mean+sd, width=0.3, color=group)) +
  geom_point(aes(x=group, y= mean, color=group),size=5,shape=3) +
  geom_point(aes(x = group, y = grade), size=2) +
  xlab("Groups") +
  ylab("Grade") +
  theme_bw() +
  theme(axis.line = element_line(colour = "black", size=.5),
        panel.border = element_rect(colour = "white"))

@

Finaly, below we compute the ANOVA tests with our data using the function \verb+aov()+.

<<t2-aov>>=
fit <- aov(grade ~ group, data = X)
summary(fit)
@

The F-statistics has value of 2.5, which is above 1 which signifies some differences in group means. However the p-value is 0.177, which is not signigicant under the 5 \% significance level and therefore we cannot reject the null hypothesis of the group means being different.\\

<<t2-modified-data, echo=FALSE, include=FALSE>>=
set.seed(352144)
X2 <- data.frame("grade" = c(4,6,8,9,5,9,10,5),
                 "group" = c("A","A","A","B","B","C","C","C"))
X3 <- data.frame("grade" = c(5,6,7,1.73,4.27,9.6,8,6.4),
                 "group" = c("A","A","A","B","B","C","C","C"))
X4 <- data.frame("grade" = c(rnorm(30,6,2),rnorm(20,3,2.83),rnorm(30,8,2.65)),
                 "group" = c(rep(c("A","B","C"),c(30,20,30))))
fit2 <- aov(grade ~ group, data = X2)
summary(fit2)

fit3 <- aov(grade ~ group, data = X3)
summary(fit3)

fit4 <- aov(grade ~ group, data = X4)
summary(fit4)

@

Case (2):\\
Suppose that the first observation in the second group was actually 9, not 1. Then, the standard deviations are the same, but the sample means are 6, 7 and 8, rather than 6, 3 and 8. In this situation, we would expect the F-test statistics to be smaller. The main reason is that the differences in the group mean are going to be smaller and therefore the nominator of the ratio, $S^2_{\text{between}}$ will be smaller and hence the whole ratio or F-statistics will be smaller. Since the F-value will be closer to 1 (or closer to the mean of F-distribution), the p-value will therefore also increase as observing statistics more extreme is more likely than before. \\

Case (3):\\
Suppose you have the same means as these data, but the sample standard deviations were 1.0, 1.8 and 1.6, instead of the actual 2.0, 2.8 and 2.6. In this situation, we would expect the F-test statistics to be larger. The main reason is that that the variation within each group have decreased and therefore the $S^2_{\text{within}}$ will decrease and as the denominator of a ratio will decrease, the whole F-test statistics will be larger. Since the F-statistics increased, the p-value will decrease (observing more extreme statistics is less likely). With such a small variances, we would could even expect the test to be significant at the 5\% significance level.\\

Case (4):\\
Suppose you have the same means and standard deviations as these data, but the sample size were 30, 20 and 30, instead of 3, 2 and 3. In this case, we would expect the F-test statistics to be larger. The main reason is the F-distribution with which we would be comparing our F-test statistics will be with 2 and 77 degrees of freedom. The groups variance has still the same number of degrees of freedom as the number of groups hasn't changed. However the residual variance now had 77 degrees of freedom compared to previously having only 5 degrees of freedom. Therefore, we would be dividing $S^2_{\text{within}}$ by 77, which would result in a signigicantly lower Mean Squared Residual compared to Mean Squared of Groups. Hence, the overall F-test statistics would significantly increase. With similar line of reasoning as before, as the F-statistics will be large, the p-value (probability of observing more extreme statistics) will be very low.\\

\section*{Task 3}

The compressive strength of concrete is being studied, and four different mixing techniques are being investigated. The following data have been collected. For each mixing technique, 4 compressive strength measurements (in pounds per square inch) have been recorded.\\

\begin{center}
\begin{tabular}{ l c c c c}
 \hline
 & \multicolumn{4}{c}{Compressive Strenght} \\
 \hline
 Mixing & 1 & 2 & 3 & 4 \\
 \hline
 1 & 3129 & 3000 & 2865 & 2890 \\
 2 & 3200 & 3300 & 2975 & 3150 \\
 3 & 2800 & 2900 & 2985 & 3050 \\
 4 & 2600 & 2700 & 2600 & 2765 \\
 \hline
\end{tabular}
\end{center}

In this task we perform the same one-factor Analysis of Variance as in previous task. We load the data to R in following way and subsequently visualise it with same techniques as before. From the plots we can clearly see that some mixing techniques are quite different to others.

<<t3-data>>=
X <- data.frame("strength"=c(3129,3000,2865,2890,
                             3200,3300,2975,3150,
                             2800,2900,2985,3050,
                             2600,2700,2600,2765),
                "mixing"=rep(c("1","2","3","4"),c(4,4,4,4)) )
@


<<t3-plots, echo=FALSE, fig.width=4, fig.height=4, out.width='.4\\linewidth'>>=
boxplot(strength ~ mixing,data=X, xlab="Mixing technique", ylab="Compressive Strength", frame=FALSE, col=c(2,3,5,6))
points(strength ~ mixing, data=X,pch=20)

Xplot <- X
out = group_by(Xplot, mixing) %>%
  summarise( count = n(), mean = mean(strength), var = var(strength), sd = sd(strength))
Xplot$mean <- c(rep(out$mean[1],4),rep(out$mean[2],4),rep(out$mean[3],4),rep(out$mean[4],4))
Xplot$sd <- c(rep(out$sd[1],4),rep(out$sd[2],4),rep(out$sd[3],4),rep(out$sd[4],4))
ggplot(Xplot) +
  geom_errorbar(aes(x= mixing, ymin= mean-sd, ymax = mean+sd, width=0.3, color=mixing)) +
  geom_point(aes(x= mixing, y= mean, color=mixing),size=5,shape=3) +
  geom_point(aes(x = mixing, y = strength), size=2) +
  xlab("Mixing technique") +
  ylab("Compressive Strength") +
  theme_bw() +
  theme(axis.line = element_line(colour = "black", size=.5),
        panel.border = element_rect(colour = "white"))

@

Below, we perform the Analysis of Variance using again \verb+aov()+ function.\\

<<t3-anova>>=
fit <- aov(strength ~ mixing, data = X)
summary(fit)
@

From the summary we can see that the F-statistics is 12.73, which is quite large for F-distribution with 3 and 12 degrees of freedom. This is also evident from the p-value being 0.0005, which is very small and we would therefore reject the null hypothesis that the group means are all the same. Therefore we can conclude there is some evidence that for at least one mixing technique, the compressive strength differs from others. But we cannot conclude which one differs. To yield this conclusion, we perform the multiple pairwise-comparison, to determine if the mean difference between specific pairs of group are statistically significant. \\

As the ANOVA test is significant, we can compute Tukey Honest Significant Differences for performing multiple pairwise-comparison between the means of groups. We use the function \verb+glht()+ [in \verb+multcomp+ package], where glht stands for general linear hypothesis tests.

<<t3-pairwise-comparison>>=
library(multcomp)
summary(glht(fit, linfct = mcp(mixing = "Tukey")))
@

In the output we can see the individual comparisons between all groups with the p-value being adjusted for multiple testing such that in controls for the family-wise error rate. We can see that the largest differenence is between mixing techniques 4 and 2, where the estimate for the difference between those two is $-490$ with p-value being significant at all levels. The differences between mixing techniques 4 - 1 and 4 - 3 are also significant at level 0.05. With the estimated difference between both of them are negative, we can conclude that the mixing technique 4 has the smallest comptessive strength then all other mixing techniques. Hence, we would not recommend to use mixing technique 4 for concrete in practice.

\section*{Task 4}
Consider the data set munich.csv. The data set contains information on therent prices of apartments in Munich.  The variables in the data set are:

rent:  net rent per month (in Euro)
area:  living area in square meters-yearc:
year of construction
location:  quality of location according to an expert assessment(0 = average location, 1 = good location, 2 = top location)


<<t4-data>>=
munich <- read.csv("munich.csv")
munich$location <- factor(munich$location)


@


<<t4-discuss>>=
par(mfrow=c(2,2))
year_agg <- aggregate(munich$rent, list(munich$yearc),
                      function(x) c(mean=mean(x), sd=sd(x)))
plot(year_agg$Group.1, year_agg$x[,"mean"], xlab="year",
     ylab="rent: mean +- 1 sd", ylim=c(0, 1300))
segments(year_agg$Group.1-0.25, year_agg$x[,"mean"]+year_agg$x[,"sd"],
          year_agg$Group.1+0.25, year_agg$x[,"mean"]+year_agg$x[,"sd"],col="grey")
segments(year_agg$Group.1-0.25, year_agg$x[,"mean"]-year_agg$x[,"sd"],
          year_agg$Group.1+0.25, year_agg$x[,"mean"]-year_agg$x[,"sd"],col="grey")
segments(year_agg$Group.1, year_agg$x[,"mean"]+year_agg$x[,"sd"],
         year_agg$Group.1,year_agg$x[,"mean"]-year_agg$x[,"sd"],col="grey")


plot(rent~area, data=munich)
plot(rent~location, data=munich)


@
From the above plots, we can already come to the reasonable conclusion that there is a relationship between rent and area, location, and year.

<<model>>=

f1 <- lm(rent~area*location + yearc, data=munich)
par(mfrow=c(2,2))
plot(f1)

@

From the 'Residuals vs Fitted' plot in the top left, we see that the assumption of zero-mean residuals holds (the trend line drifts away from zero as the fitted values go up, but this can be explained by the lack of data in that area of the plot). However, it could be argued that the variance of the errors increase with the fitted values, but this is quite hard to be sure of with this graph alone.\\

The 'Normal Q-Q' plot is perhaps a little concerning with the tails drifting away from the 'expected' line.\\

In the 'Scale-Location' graph, we again have evidence for heteroscedastic residuals, as the trend variance of residuals appears to get larger as the fitted values get larger (which we also observed in the first plot). However, it could be argued again that this major deviation of the trend line from the expected behaviour is due to a handful of points. However, there is mounting evidence for heterscedastic resisualds, so perhaps a variable transformation is required.\\

Finally, in the 'Residuals vs Leverage' plot, we can see that the majority of datapoints have low leverage and a relatively small Cook's distance. There is just one point, row #3909, which has a high leverage as well as a high Cook's distance. This datapoint should be looked at in more detail as the analysis progresses beyond this initial phase.\\

<<summary>>=
summary(f1)

@

The model results confirm our reasoning that there is a relationship between the three variables (area, location, and year) and rent. Both area and year have a very low p-value. However, location a high p-value which would at first glance suggest that it doesn't have relationship with the rent value, however we can see that the interaction between location and area has a low p-value, so it is feasible that the 'information' given by location is captured in this interaction. It is good practice to leave this 'main effect' of location in the model still, as we will be including its interaction with area.

<<compare>>=

f2 <- lm(rent~area, data=munich)
summary(f2)
@



<<anova>>=

anova(f2, f1)

@

This analysis of variance shows that the larger model (i.e. including location, year, and the interaction between area and location) is significantly better than the smaller model of just including area. The Residual Sum of Squares is significantly lower, leading to a p-value that signifies a signicant result.


\end{document}
