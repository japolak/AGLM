%% Do not edit preable unless you know what you are doing.
\documentclass{article}\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{dsfont}
\usepackage{indentfirst}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}

% No indents in whole document
\setlength\parindent{0pt}

% Gaps between item is itemize
\setitemize{itemsep=-1em,topsep=0.5em,parsep=0em,partopsep=0pt}

% Margins of the document
\geometry{verbose,tmargin=2.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}

% Header and footer for all pages
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\rfoot{Page \thepage}

% Header and footer for first page
\fancypagestyle{plain}{%
  \renewcommand{\headrulewidth}{0pt}%
  \fancyhf{}%
  \rhead{ETH Zurich}
  \lhead{Applied Generalized Linear Models \\ Spring Semester 2020}
  \rfoot{Page \thepage}
}

% Global Settings of R-code


% Mathematics Operators for Ease of Notation
\DeclareMathOperator{\SSR}{SSR}
\DeclareMathOperator{\SSReg}{SSReg}
\DeclareMathOperator{\SST}{SST}

% Bad Box settings
\hbadness=10000
\vbadness=\maxdimen
\vfuzz=30pt
\hfuzz=30pt


%% ---------- BEGIN DOCUMENT -----------------------------------------------
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
%\SweaveOpts{concordance=TRUE}

%% Tiltle
\title{Assignment 2}
\author{Milan Kuzmanovic, Mark McMahon \\ Martin Kotuliak, Jakub Polak}
\date{\today}
\maketitle

\section*{Task 1}

The following table contains the estimates of a logistic regression model.



\begin{center}
\begin{tabular}{ l r r r r r r r}
 \hline
            & & & & & & \multicolumn{2}{c}{95\% C.I. for OR}\\
 \hline
            & Est.            & s.e.              & z         & p-value & OR & lower & higher\\
 \hline
 $X_1$      & -0.868 & {\bf 0.367}  & -2.365     & 0.018 & {\bf 0.42} & 0.205 & 0.865 \\
 $X_2$      & 2.404  & 0.601 & 4.000     & <0.001 & {\bf 11.067} & {\bf 3.408} & {\bf 35.943} \\
 $X_3$      & {\bf -3.604}  & {\bf 0.511} & {\bf -7.059}    & <0.001 & {\bf 0.027} & 0.010 & 0.074 \\
 \hline
\end{tabular}
\end{center}

Fill in the missing information
(Please report formulas and computation.)
\begin{align*}
\text{s.e.}_{X_1}=\frac{X_1}{z_{X_1}}=\frac{-0.868}{-2.365}=0.367\\
\text{OR}_{X_1}=e^{X_1}=e^{-0.868}=0.42\\
\text{OR}_{X_2}=e^{X_2}=e^{2.404}=11.067\\
\text{lower}_{X_2}=e^{X_2 - z_{0.975} \times \text{s.e.}_{X_2}}=e^{2.404 - 1.96\times 0.601}=3.408\\
\text{lower}_{X_2}=e^{X_2 + z_{0.975} \times \text{s.e.}_{X_2}}=e^{2.404 + 1.96\times 0.601}=35.943\\
X_3=\frac{\ln \text{lower}_{X_3}+ \ln \text{higher}_{X_3}}{2} = \frac{0.01+ \ln 0.074}{2} = -3.604\\
\text{s.e.}_{X_3}=\frac{- \ln \text{lower}_{X_3}+ \ln \text{higher}_{X_3}}{2 \times z_{0.975}} = \frac{- 0.01+ \ln 0.074}{2 \times 1.96} = 0.511\\
\text{z}_{X_3}=\frac{X_3}{\text{s.e.}_{X_3}}=\frac{-3.604}{0.511}=-7.059\\
\text{OR}_{X_3}=e^{X_3}=e^{-3.604}=0.027\\
\end{align*}

\section*{Task 2}

During the lecture, we have considered three systems of hypotheses for the parameters of the MNRM:
\begin{enumerate}
\item $H_0 : \beta_{jm} = 0 \quad vs. \quad H_1: \beta_{jm} \neq 0$
\item $H_0 : \beta_{j1} = ... = \beta_{j(M-1)} = 0 \quad vs. \quad H_1: at \ least \ one \ \beta_{jm} \neq 0, \ \forall m$
\item $H_0 : \beta_{j1} = ... = \beta_{j(M-1)} = 0 \quad vs. \quad H_1: at \ least \ one \ \beta_{jm} \neq 0, \ \forall m, j$
\end{enumerate}
Could you specify another pair of hypotheses $H_0$ and $H_1$ for the parameters of the MNRM that we might want to test? Justify your answer. \\

Firstly, it is important to explain which hypotheses can be tested, i.e. for which hypotheses is there a procedure to define a valid (asymptotic) test. The above  hypotheses are tested either based on the asymptotic normality of the maximum likelihood estimator, or on the likelihood ratio statistic whose logarithm is asymptotically Chi-square distributed. It is important to notice that we can extend the hypothesis test $H_0 : \beta_{jm} = 0 \quad vs. \quad H_1: \beta_{jm} \neq 0$ that is based on the asymptotic normality of the ML estimator to a joint test for any set of parameters. We can do this because the ML estimator is jointly normally distributed, so any subset of the set of all parameters is also jointly normally distributed, which means that by taking the quadratic form of the estimator minus hypothesized value and covariance matrix estimated by the inverse of the Fisher information matrix, we can define a Chi-square test statistic for any null hypotheses that an arbitrary subset of parameters is simultaneously equal to some specific hypothesized values (generally to zero). Therefore, we can in general construct a test for any joint hypothesis on the parameters. Now comes the question of which hypotheses might be of interest, i.e. which questions we might want an answer to? We propose several possible questions of interest defined by the below hypotheses:
\\
\begin{enumerate}
\item $H_0 : \beta_{jm} = c \quad vs. \quad H_1: \beta_{jm} \neq c$ where $c \in \mathbb{R} $ is some constant. Here we have the same logic of the test as for $c = 0$. In fact, this is just a generalization that is useful if we want to answer more specific questions about the parameters. We might be interested in testing not just for the existence of a significant effect ($c = 0$ case), but also about specific magnitude of the effect (e.g. $c = 1$).
\item $H_0 : \beta_{jm} = 0 \quad vs. \quad H_1: \beta_{jm} > 0$. We can change the alternative hypothesis in case we want to test whether there is a positive effect. Here, we assume that the parameter can't be negative, only zero under the null hypothesis or positive under the alternative hypothesis. This version of the test with different, so-called one-sided alternative, gives us more power in detecting positive (or negative in case $H_1: \beta_{jm} < 0$) effects because we assume that the effect is either zero or positive, so the rejection region for given significance $\alpha$ is two times larger on the positive (or negative) side compared to the case with the two-sided alternative.
\item $H_0 : \beta_{j1} = ... = \beta_{j(M-1)} = c \quad vs. \quad H_1: at \ least \ one \ \beta_{jm} \neq c, \ \forall m$, where $c \in \mathbb{R} $ is some constant. This is a variation of the second hypothesis test that was introduced in the lecture. Instead of testing if all of the parameters associated with variable j are zero, which would mean that variable j has no significant influence on the outcome variable, we can test that all of the parameters associated with variable j have the same effect $c \in \mathbb{R} $, on the odds relative to the reference group M. For example, having three groups, and group 3 as the reference, we might be interested if the increase in variable j results in the same change in the odds for group 1 relative to group 3 and for group 2 relative to group 3.
\item $H_0 : \beta_{1m} = ... = \beta_{pm} = 0 \quad vs. \quad H_1: at \ least \ one \ \beta_{jm} \neq 0, \ \forall j$. This hypothesis test would test if the variables in the model have any influence on the odds between group m and reference group M. We might be interested in this question if we suspect that for a specific group m the variables in the model don't affect the odds of m happening relative to M happening.
\item $H_0 : \beta_{j_{1}m} - \beta_{j_{2}m} =  0 \quad vs. \quad H_1: \beta_{j_{1}m} - \beta_{j_{2}m} \neq 0$ This hypothesis test would test if the effect of the variable $j_1$ and the variable $j_2$ on the odds of group m relative to reference group M is the same.
\end{enumerate} \\
There are many more possibilities for hypotheses tests. So, if we have a question of interest we can define a hypothesis test for that question, and likelihood theory allows us to construct asymptotic pivots for testing all sorts of hypotheses and thus answering all sorts of questions of interest.


\end{document}
