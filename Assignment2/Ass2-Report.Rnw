%% Do not edit preable unless you know what you are doing.
\documentclass{article}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{dsfont}
\usepackage{indentfirst}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{amsmath}

% No indents in whole document
\setlength\parindent{0pt}

% Gaps between item is itemize
\setitemize{itemsep=-1em,topsep=0.5em,parsep=0em,partopsep=0pt}

% Margins of the document
\geometry{verbose,tmargin=2.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}

% Header and footer for all pages
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\rfoot{Page \thepage}

% Header and footer for first page
\fancypagestyle{plain}{%
  \renewcommand{\headrulewidth}{0pt}%
  \fancyhf{}%
  \rhead{ETH Zurich}
  \lhead{Applied Generalized Linear Models \\ Spring Semester 2020}
  \rfoot{Page \thepage}
}

% Global Settings of R-code
<<setup, include=FALSE, cache=FALSE>>=
# Install these packages if you don't have them already
# install.packages("knitr","dplyr","ggplot2","multcomp")
library(dplyr)
library(multcomp)
library(ggplot2)
library(knitr)
# set global chunk options
opts_chunk$set(fig.path='figures/plot-', fig.align='center', fig.show='hold')
options(formatR.arrow=TRUE,width=90)
@

% Mathematics Operators for Ease of Notation
\DeclareMathOperator{\SSR}{SSR}
\DeclareMathOperator{\SSReg}{SSReg}
\DeclareMathOperator{\SST}{SST}

% Bad Box settings
\hbadness=10000
\vbadness=\maxdimen
\vfuzz=30pt
\hfuzz=30pt


%% ---------- BEGIN DOCUMENT -----------------------------------------------
\begin{document}
%\SweaveOpts{concordance=TRUE}

%% Tiltle
\title{Assignment 2}
\author{Milan Kuzmanovic, Mark McMahon \\ Martin Kotuliak, Jakub Polak}
\date{\today}
\maketitle

\section*{Task 1}

The following table contains the estimates of a logistic regression model.

<<t1-comps, include=FALSE>>=
x1 <- -0.868
z_x1 <- -2.365
x1_se <- x1 / z_x1
or_x1 <- exp(x1)



x2 <- 2.404
or_x2 <- exp(x2)
x2_se <- 0.601

l_ci_x2 <- exp(x2-qnorm(0.975)*x2_se)
h_ci_x2 <- exp(x2+qnorm(0.975)*x2_se)

l_ci_x3 <- 0.010
h_ci_x3 <- 0.074

x3 <- (log(l_ci_x3) + log(h_ci_x3)) / 2
x3_se <- (- log(l_ci_x3) + log(h_ci_x3)) /(2*qnorm(0.975))
z_x3 <- x3/x3_se

or_x3 <- exp(x3)
@

\begin{center}
\begin{tabular}{ l r r r r r r r}
 \hline
            & & & & & & \multicolumn{2}{c}{95\% C.I. for OR}\\
 \hline
            & Est.            & s.e.              & z         & p-value & OR & lower & higher\\
 \hline
 $X_1$      & -0.868 & {\bf \Sexpr{round(x1_se, 3)}}  & -2.365     & 0.018 & {\bf \Sexpr{round(or_x1, 3)}} & 0.205 & 0.865 \\
 $X_2$      & 2.404  & 0.601 & 4.000     & <0.001 & {\bf \Sexpr{round(or_x2, 3)}} & {\bf \Sexpr{round(l_ci_x2, 3)}} & {\bf \Sexpr{round(h_ci_x2, 3)}} \\
 $X_3$      & {\bf \Sexpr{round(x3, 3)}}  & {\bf \Sexpr{round(x3_se, 3)}} & {\bf \Sexpr{round(z_x3, 3)}}    & <0.001 & {\bf \Sexpr{round(or_x3, 3)}} & 0.010 & 0.074 \\
 \hline
\end{tabular}
\end{center}

Fill in the missing information
(Please report formulas and computation.)
\begin{align*}
\text{s.e.}_{X_1}=\frac{X_1}{z_{X_1}}=\frac{\Sexpr{round(x1, 3)}}{\Sexpr{round(z_x1, 3)}}=\Sexpr{round(x1_se, 3)}\\
\text{OR}_{X_1}=e^{X_1}=e^{\Sexpr{round(x1, 3)}}=\Sexpr{round(or_x1, 3)}\\
\text{OR}_{X_2}=e^{X_2}=e^{\Sexpr{round(x2, 3)}}=\Sexpr{round(or_x2, 3)}\\
\text{lower}_{X_2}=e^{X_2 - z_{0.975} \times \text{s.e.}_{X_2}}=e^{\Sexpr{round(x2, 3)} - \Sexpr{round(qnorm(0.975), 3)}\times \Sexpr{round(x2_se, 3)}}=\Sexpr{round(l_ci_x2, 3)}\\
\text{higher}_{X_2}=e^{X_2 - z_{0.025} \times \text{s.e.}_{X_2}}=e^{X_2 + z_{0.975} \times \text{s.e.}_{X_2}}=e^{\Sexpr{round(x2, 3)} + \Sexpr{round(qnorm(0.975), 3)}\times \Sexpr{round(x2_se, 3)}}=\Sexpr{round(h_ci_x2, 3)}\\
X_3=\frac{\ln (\text{lower}_{X_3})+ \ln (\text{higher}_{X_3})}{2} = \frac{\ln (\Sexpr{round(l_ci_x3, 3)}) + \ln (\Sexpr{round(h_ci_x3, 3)})}{2} = \Sexpr{round(x3, 3)}\\
\text{s.e.}_{X_3}=\frac{- \ln (\text{lower}_{X_3}) + \ln (\text{higher}_{X_3})}{2 \times z_{0.975}} = \frac{- \ln (\Sexpr{round(l_ci_x3, 3)}) + \ln (\Sexpr{round(h_ci_x3, 3)})}{2 \times \Sexpr{round(qnorm(0.975), 3)}} = \Sexpr{round(x3_se, 3)}\\
\text{z}_{X_3}=\frac{X_3}{\text{s.e.}_{X_3}}=\frac{\Sexpr{round(x3, 3)}}{\Sexpr{round(x3_se, 3)}}=\Sexpr{round(z_x3, 3)}\\
\text{OR}_{X_3}=e^{X_3}=e^{\Sexpr{round(x3, 3)}}=\Sexpr{round(or_x3, 3)}\\
\end{align*}

\section*{Task 2}

During the lecture, we have considered three systems of hypotheses for the parameters of the MNRM:
\begin{enumerate}
\item $H_0 : \beta_{jm} = 0 \quad vs. \quad H_1: \beta_{jm} \neq 0$
\item $H_0 : \beta_{j1} = ... = \beta_{j(M-1)} = 0 \quad vs. \quad H_1: at \ least \ one \ \beta_{jm} \neq 0, \ \forall m$
\item $H_0 : \beta_{j1} = ... = \beta_{j(M-1)} = 0 \quad vs. \quad H_1: at \ least \ one \ \beta_{jm} \neq 0, \ \forall m, j$
\end{enumerate}
Could you specify another pair of hypotheses $H_0$ and $H_1$ for the parameters of the MNRM that we might want to test? Justify your answer. \\

Firstly, it is important to explain which hypotheses can be tested, i.e. for which hypotheses is there a procedure to define a valid (asymptotic) test. The above  hypotheses are tested either based on the asymptotic normality of the maximum likelihood estimator, or on the likelihood ratio statistic whose logarithm is asymptotically Chi-square distributed. It is important to notice that we can extend the hypothesis test $H_0 : \beta_{jm} = 0 \quad vs. \quad H_1: \beta_{jm} \neq 0$ that is based on the asymptotic normality of the ML estimator to a joint test for any set of parameters. We can do this because the ML estimator is jointly normally distributed, so any subset of the set of all parameters is also jointly normally distributed, which means that by taking the quadratic form of the estimator minus its hypothesized value and covariance matrix estimated by the inverse of the Fisher information matrix, we can define a Chi-square test statistic for any null hypotheses that an arbitrary subset of parameters is simultaneously equal to some specific hypothesized values (generally to zero). Therefore, we can in general construct a test for any joint hypothesis on the parameters. Now comes the question of which hypotheses might be of interest, i.e. which questions we might want an answer to? We propose several possible questions of interest defined by the below hypotheses:
\\
\begin{enumerate}
\item $H_0 : \beta_{jm} = c \quad vs. \quad H_1: \beta_{jm} \neq c$ where $c \in \mathbb{R} $ is some constant. Here we have the same logic of the test as for $c = 0$. In fact, this is just a generalization that is useful if we want to answer more specific questions about the parameters. We might be interested in testing not just for the existence of a significant effect ($c = 0$ case), but also about specific magnitude of the effect (e.g. $c = 1$).
\item $H_0 : \beta_{jm} = 0 \quad vs. \quad H_1: \beta_{jm} > 0$. We can change the alternative hypothesis in case we want to test whether there is a positive effect. Here, we assume that the parameter can't be negative, only zero under the null hypothesis or positive under the alternative hypothesis. This version of the test with different, so-called one-sided alternative, gives us more power in detecting positive (or negative in case $H_1: \beta_{jm} < 0$) effects because we assume that the effect is either zero or positive, so the rejection region for given significance $\alpha$ is two times larger on the positive (or negative) side compared to the case with the two-sided alternative.
\item $H_0 : \beta_{j1} = ... = \beta_{j(M-1)} = c \quad vs. \quad H_1: at \ least \ one \ \beta_{jm} \neq c, \ \forall m$, where $c \in \mathbb{R} $ is some constant. This is a variation of the second hypothesis test that was introduced in the lecture. Instead of testing if all of the parameters associated with variable j are zero, which would mean that variable j has no significant influence on the outcome variable, we can test that all of the parameters associated with variable j have the same effect $c \in \mathbb{R} $, on the odds relative to the reference group M. For example, having three groups, and group 3 as the reference, we might be interested if the increase in variable j results in the same change in the odds for group 1 relative to group 3, and for group 2 relative to group 3.
\item $H_0 : \beta_{1m} = ... = \beta_{pm} = 0 \quad vs. \quad H_1: at \ least \ one \ \beta_{jm} \neq 0, \ \forall j$. This hypothesis test would test if the variables in the model have any influence on the odds between group m and reference group M. We might be interested in this question if we suspect that for a specific group m the variables in the model don't affect the odds of m happening relative to M happening.
\item $H_0 : \beta_{j_{1}m} - \beta_{j_{2}m} =  0 \quad vs. \quad H_1: \beta_{j_{1}m} - \beta_{j_{2}m} \neq 0$ This hypothesis test would test if the effect of the variable $j_1$ and the variable $j_2$ on the odds of group m relative to reference group M is the same.
\end{enumerate}

There are many more possibilities for hypotheses tests. So, if we have a question of interest we can define a hypothesis test for that question, and likelihood theory allows us to construct asymptotic pivots for testing all sorts of hypotheses and thus answering all sorts of questions of interest.

\newpage

\section*{Task 3}
\textbf{Q1. }

The website blablabla.com sells magazine subscriptions. The related company is going to plan a large e-mail marketing campaign. All of the e-mails that will be sent will go to customers that have previously bought a magazine subscription at blablabla.com and who have not opted out of receiving e-mails. \\

The magazines advertised in each e-mail will be automatically selected for each customer when the e-mail is generated in order to maximize the probability that the customer will buy. The website blablabla.com will only include ads for three magazines in each e-mail in a row at the top of the message so that it is likely that the ads will appear in the e-mail preview (and therefore actually be viewed without the receiver actually having to open the e-mail). Moreover, the managers believe that including more ads is ineffective. \\

To evaluate the efficacy of the campaign, the company run an experiment. They sent 673 e-mails to customers containing the ad for the “Art with you” magazine and recorded whether or not the customer purchased this magazine. \\

The company has also collected data on the customers by matching the in- formation provided by third party data (which can be purchased from data sources such as the credit scoring agencies) and the recipient of the e-mails when he/she made a purchase at blablabla.com. \\

\textbf{Q1. Estimate a logistic regression model, with Buy as the dependent variable and Gender, Not Employed, Income and Own as explanatory variables. Is there any explanatory variable you would remove from the model? On the basis of which test? Remove the variable(s) and named the resulting model as Model 1.}

<<>>=
magazine <- read.csv("magazine.csv",header=TRUE)
model_0 <- glm(Buy~Is.Female+Unemployed+Income+Own,family="binomial", data = magazine)
summary(model_0)
drop1(model_0,test="Chisq")
@


From the above summary of glm model, it seems that the variable “Unemployed” is not significant at 5\% level (p-value $= 0.8045 >> 0.05$ ). The summary function performs a Wald test. This is the result of a two sided test testing of hypothesis $H_0 : \beta_{Unemployed} = 0$ vs. $H_1: \beta_{Unemployed} \neq 0.$ The test statistic is computed as $W = Estimate/Std.Error$ and compares it to standard normal distribution. \\

The same result would be found by using \textbf{drop1} function and performing Likelihood ratio test. LRT compares whether the difference in log likelihoods of a full model and a nested smaller model is significant. The model that fits better has a higher log-likelihood so the test statistic is computed as $D = 2(LL_0 - LL_{drop1})$ follows a Chisq distribution with the degrees of freedom being the difference in number of variables. It is indicative of an input variable that is in some way important to include in a good model. We see that all smaller models are significantly different from the full model, besides the model without Unemployment. Hence, the model without Unemployment is not significantly different from the model with Unemployment, which means the smaller model without it is just as good. \\

The \textbf{drop1} function also shows AIC criterion for the models obtained by dropping one term. Leaving all the variables in the model gives an AIC of $234.53$. The AIC can be improved (ie. decreased) by dropping “Unemployed”, resulting in a new AIC of $232.59$. \\

<<>>=
model1 <- glm(Buy~Is.Female+Income+Own,family="binomial",data = magazine)
summary(model1)
@

Now all the parameters are significantly different from 0 (at $\alpha = 0.05$) in the summary.\\

\textbf{Q2. Add to Model 1 the variables PrevArt and PrevCinema. Name the re- sulting model as Model2. Compare Model 1 and Model 2 using an ap- propriate test and comment on the results.}

<<>>=
model2 <- update(model1,.~.+Prev.Art.Mag+Prev.Cinema.Mag)
summary(model2)
anova(model1,model2,test="Chisq")
@

We compare the two models with the anova function and a chi-squared-test. The statistics $G$ takes value $6.7045$, has $\chi^2$ distribution and p-value $0.035$. Thus at significance level $\alpha = 0.05$ we reject $H_0 : \beta_{PrevArt} = \beta_{PrevCin} = 0$ in favor of the alternative $H_1 :$ at least one of $\beta_{PrevArt}, \beta_{PrevCin}$ is different from 0, and conclude that Model 2 including PrevArt and PrevCin has a better fit than Model 1. \\

Remark: from the summary output of Model 2 one sees that PrevCin is probably not significantly different from 0. The following checks the Model 2a which adds only “PrevArt” to Model 1.

<<>>=
model2a <- update(model1,.~.+Prev.Art.Mag)
summary(model2a)
anova(model1,model2a,test="Chisq")
anova(model2a,model2,test="Chisq")
@

One sees that the step from Model 1 to Model 2a (adding PrevArt) yields significant (simultaneous) statistics at $\alpha = 0.05$, whereas adding PrevCin (Model 2a to Model 2) does not test as significant at $\alpha = 0.05$: we cannot reject $H_0 : \beta_{PrevCin} = 0$. Thus probably only adding PrevArt would be a better choice than adding PrevArt and PrevCin simultaneously. It is essential to notice, that this is not a contradiction to the above test results of $H_0 : \beta_{PrevArt} = \beta_{PrevCin} = 0$, as the alternative hypothesis is “at least one $\beta$ is not equal to 0”. Thus, $\beta_{PrevCin}$ could still be equal 0 given this $\alpha$ and data. One should also note that in this kind of several hypothesis testing, we should be adjusting to the p-values accroding to multiple testing procedure or control for a family wise error rate.\\


\textbf{Q3. The data set includes many potential explanatory variables. Automatic procedures to select the “best” model are implemented in any software. One of this procedure is called “backward elimination”. Select the “best” model. Interpret the parameters describing the re-lation between Buy and all the selected explanatory variables.}

<<blabla, results=FALSE>>=
fullmodel<- glm(Buy ~.,family='binomial', data=magazine)
mod.fin <- step(fullmodel, direction = 'backward')
@

<<>>=
summary(mod.fin)
odds_inc <- exp(1000*mod.fin$coefficients["Income"])
odds_fem <- exp(mod.fin$coefficients["Is.Female"])
odds_ret <- exp(mod.fin$coefficients["Is.Retired"])
odds_res <- exp(mod.fin$coefficients["Residence.Length"])
odds_dual <- exp(mod.fin$coefficients["Dual.Income"])
odds_min <- exp(mod.fin$coefficients["Minors"])
odds_own <- exp(mod.fin$coefficients["Own"])
odds_house <- exp(mod.fin$coefficients["House"])
odds_whi <- exp(mod.fin$coefficients["White"])
odds_eng <- exp(mod.fin$coefficients["English"])
odds_prart <- exp(mod.fin$coefficients["Prev.Art.Mag"])
@

The step()-function using the backward eliminations selects the following parameters as the best model based on the lowest AIC of $208.04$:\\

\textbf{Income} $\beta_{Income}$ = \Sexpr{mod.fin$coefficients["Income"]}: An increase in income by 1000, increases multiplicatively the odds of buying the magazine by the factor $\exp(\beta_{Income} * 1000)= \Sexpr{odds_inc}$.\\

\textbf{Is.Female} $\beta_{Is.Female}$ = \Sexpr{mod.fin$coefficients["Is.Female"]}: The customer being female increases multiplicatively the odds of buying the magazine by the factor $\exp(\beta_{Is.Female})=\Sexpr{odds_fem}$, compared to customer being male. \\

\textbf{Is.Retired} $\beta_{Is.Retired}$ = \Sexpr{mod.fin$coefficients["Is.Retired"]}: The customer being retired decreases multiplicatively the odds of buying the magazine by the factor $\exp(\beta_{Is.Retired})=\Sexpr{odds_ret}$, compared to customer not being retired. \\

\textbf{Residence.Length} $\beta_{Residence.Length}$ = \Sexpr{mod.fin$coefficients["Residence.Length"]}: An increase of the length of residence in the current city by 1 year, increases multiplicatively the odds of buying the magazine by the factor $\exp(\beta_{Residence.Length})=\Sexpr{odds_res}$.\\

\textbf{Dual.Income} $\beta_{Dual.Income}$ = \Sexpr{mod.fin$coefficients["Dual.Income"]}: If the household considered has a dual income, the odds of buying the magazine are increased multiplicatively by the factor $\exp(\beta_{Dual.Income})=\Sexpr{odds_dual}$, compared to household not having dual income. \\

\textbf{Minors} $\beta_{Minors}$ = \Sexpr{mod.fin$coefficients["Minors"]}: If children under the age of 18 are living in the household of the customer, the odds of buying the magazine are increased multiplicatively by the factor $\exp(\beta_{Minors})=\Sexpr{odds_min}$, compared to household without children under 18. \\

\textbf{Own} $\beta_{Own}$ = \Sexpr{mod.fin$coefficients["Own"]}: If the customer owns his/her residence, the odds of buying the magazine are increased multiplicatively by the factor $\exp(\beta_{Own})=\Sexpr{odds_own}$, compared to when the customer does not own his/her residence.\\

\textbf{House} $\beta_{House}$ = \Sexpr{mod.fin$coefficients["House"]}: If the customer reside in a single family house, the odds of buying the magazine are decreased multiplicatively by the factor $\exp(\beta_{House})= \Sexpr{odds_house}$, compared to if the customer doesn't reside in a single family house.\\

\textbf{White} $\beta_{White}$ = \Sexpr{mod.fin$coefficients["White"]}: If the customer’s race is white, the odds of buying the magazine are increased multiplicatively by the factor $\exp(\beta_{White})=\Sexpr{odds_whi}$, compared to if the customer's race is not white.\\

\textbf{English} $\beta_{English}$ = \Sexpr{mod.fin$coefficients["English"]}: If the customer’s native language is English, the odds of buying the magazine are increased multiplicatively by the factor $\exp(\beta_{English})= \Sexpr{odds_eng}$, compared to when customer's native language in not English. \\

\textbf{Prev.Art.Mag} $\beta_{Prev.Art.Mag}$ = \Sexpr{mod.fin$coefficients["Prev.Art.Mag"]}: If the customer has previously bought an art magazine, the odds of buying the magazine are increased multiplicatively by the factor $\exp(\beta_{Prev.Art.Mag}) =\Sexpr{odds_prart}$, compared to if the customer did not previously buy an art magazine. \\

Remark: This interpretation is only valid if we're controlling for all other variables. This means that we change only one variable in question to obtain above interpretation, while holding all other variables constant.

\section*{Task 4}
\textbf{Q1. Write down the formulation of the MNRM assuming “fish” as reference category}\\

The general form can be written as:

\begin{multline*}
logit[\pi_m(x)] = log[\frac{\pi_m(x)}{\pi_M(x)}] = \beta_{0m} + \beta_{1m}X_{lakeHancock} + \beta_{2m}X_{lakeOklawaha}\\ + \beta_{3m}X_{lakeTrafford} + \beta_{4m}X_{sexMale} +\beta_{5m}X_{sizeSmall}
\end{multline*}

Where m $\in$ \{invertebrate, reptile, bird, other\} and M = fish. \\

For example, using reptile we have:

\begin{multline*}
logit[\pi_{reptile}(x)] = log[\frac{\pi_{reptile}(x)}{\pi_{fish}(x)}] = \beta_{0,reptile} + \beta_{1,reptile}X_{lakeHancock} + \beta_{2,reptile}X_{lakeOklawaha}\\ + \beta_{3,reptile}X_{lakeTrafford} + \beta_{4,reptile}X_{sexMale} +\beta_{5,reptile}X_{sizeSmall}
\end{multline*} \\

\textbf{Q2. Estimate the model using R. Interpret the parameters of the log-odds of reptile vs fish}\\
<<t4-q2_data, include=FALSE>>=
library(nnet)
library(tidyverse)

data <- read.csv("alligator.csv")
data$food <- relevel(data$food, "fish")
@

<<t4-q2, include=TRUE>>=
fit <- multinom(food ~ lake + sex + size, data, weights = count)

@

<<t4-table, include=FALSE>>=
output <- summary(fit)
z <- output$coefficients/output$standard.errors
p <- (1 - pnorm(abs(z), 0, 1))*2
resbird <-cbind(output$coefficients[1,],output$standard.errors[1,],z[1,],p[1,])
resinvert <- cbind(output$coefficients[2,],output$standard.errors[2,],z[2,],p[2,])
resother <- cbind(output$coefficients[3,],output$standard.errors[3,],z[3,],p[3,])
resreptile <- cbind(output$coefficients[4,],output$standard.errors[4,],z[4,],p[4,])

summary <- rbind(resbird, resinvert, resother, resreptile)
summary <- round(summary,digits=3)
colnames(summary) <- c("Est.","Std. Errors","z stat","p value")

summary <- data.frame(logit=c(rep("Bird vs. Fish",6),rep("Invertebrate vs. Fish",6),rep("Other vs. Fish",6),rep("Reptile vs. Fish",6)),
                  param=rep(rownames(resbird),4) , summary)
rownames(summary) <- NULL
@

<<t4-q2_summary, include=TRUE>>=

print(summary)

@


Above is the table to estimated parameters for the MNRM, using fish as the reference category. Below is a subset of this table with just the parameters for reptile vs fish, and their respective statistics:\\


<<t4-q2_summary_subset, include=TRUE>>=
print(summary[summary[,"logit"]=="Reptile vs. Fish",])
@

Now we can interpret these as follows:\\


$\beta_{0,reptile}$ = -2.859: This indicates that for a large female alligator from lake George the odds that the food of choice of is reptile vs. fish are $e^{-2.859} = 0.0573$\\


$\beta_{1,reptile}$ = 1.129: This indicates that when the alligator lives at the Lake Hancock the odds that the food of choice is reptile vs. fish increase multiplicatively by the factor $e^{1.129} = 3.0925$, compared to when the alligator lives at the Lake George, while controlling for all other variables in the model (size and sex). The p-value for this level is given as 0.344, but this should not lead us to believe that this whole variable is not significant as this is just one level in the categorical variable. We would need to do a global test for significance by removing this variable from the model fit and comparing the two models to get a reading on the significance of this variable. The same can be said for the remaining two levels of this lake variable below.\\


$\beta_{2,reptile}$ = 2.530: This indicates that when the alligator lives at the Lake Oklawaha the odds that the food of choice is reptile vs. fish increase multiplicatively by the factor $e^{2.530} = 12.553$, compared to when the alligator lives at the Lake George, while controlling for all other variables in the model (size and sex). \\


$\beta_{3,reptile}$ = 3.061: This indicates that when the alligator lives at the Lake Trafford the odds that the food of choice is reptile vs. fish increase multiplicatively by the factor $e^{3.061} = 21.349$, compared to when the alligator lives at the Lake George, while controlling for all other variables in the model (size and sex). \\

$\beta_{4,reptile}$ = -0.628: This indicates that when the alligator is male the odds that the food of choice is reptile vs. fish decrease multiplicatively by the factor $e^{-0.628} = 0.5336$, compared to when the alligator is female, while controlling for all other variables in the model (size and lake). The p-value for this variable can be read directly as 0.360 which, if using the usual 5\% level of signifcance, would lead to us failing to reject the hypothesis that the value of this parameter is equal to zero, given all the other variables in the model.  \\

$\beta_{5,reptile}$ = -0.557: This indicates that when the alligator is small the odds that the food of choice is reptile vs. fish decrease multiplicatively by the factor $e^{-0.557} = 0.5729$, compared to when the alligator is large, while controlling for all other variables in the model (sex and lake). Again, this p-value can be read directly as 0.389 and again, this would lead to us failing to reject the hypothesis that the value of this parameter is equal to zero, given all the opther variables in the model.\\



\textbf{Q3. What is the odds ratio of bird versus other for an alligator of small size relative to an alligator of large size?  Interpret this odds ratio}\\

The odds ratio here can be given by:

\[OR = \frac{\frac{P(Y=bird | alligator=small)}{P(Y=bird | alligator = large)}}{\frac{P(Y=other | alligator=small)}{P(Y=other | alligator = large)}} = \frac{e^{\beta_{5, bird}}}{e^{\beta_{5, other}}} = e^{\beta_{5, bird} - \beta_{5, other}} = e^{-0.730 - 0.291} = e^{-1.021} = 0.360\]


This indicates that when the alligator is small the odds that the food of choice is bird vs. other decrease multiplicatively by the factor $0.360$, compared to when the alligator is large, while controlling for all other variables in the model (sex and lake).  \\

\textbf{Q4. Test the global significance of the variable lake.}\\

<<t4-q4, include=TRUE>>=

fit2 <- update(fit, ~ . - lake)
anova(fit, fit2, test="Chisq")
@

Here we use the update() function to remove the `lake' variable from the model, and we then produce an anova table to compare the fit of the two models. We are testing the following:

\[H_0: \beta_{1,m} = \beta_{2,m} = \beta_{3,m} = 0 ; \forall m \in \{invertebrate, reptile, bird, other\}\]

versus

\[H_1: \beta_{jm} \neq 0 ; \text{ for at least one combination of } j=1, 2, 3 \text{ and } m \in \{invertebrate, reptile, bird, other\}\]

In words, we are testing the null hypothesis that the beta values for each level of the categorical variable `lake' are equal to zero for each of the Y-values, versus the alternative that at least one of the beta values is not equal to zero.\\


The testing distribution that is used is the chi-squared distribution and in this case we are testing with 12 degrees of freedom. The test statistic has a value of 50.318, which leads to a p-value of 1.228e-06. This indicates a strong evidence against the null hypothesis, and at the significance level $\alpha = 0.05$, we can reject the null hypothesis that the variable $lake$ is not significant in explaining the primary food choice of alligators. \\

\textbf{Q5. Test the fit of the model}\\
Here we fit a model with just the intercept and compare the deviance of the full model with the deviance of this intercept-only model. The hypotheses we are testing is:

\[H_0: \beta_{jm} = 0 ; \forall j=1,...,6; m \in \{invertebrate, reptile, bird, other\}\]

and

\[H_1: \beta_{jm} \neq 0 ; \text{ for at least one } j=1,...,6; m \in \{invertebrate, reptile, bird, other\}\]

In words, we are testing the null hypothesis that the beta values for each variable in the dataset is equal to zero for each of the Y-values, versus the alternative that at least one of the beta values is not equal to zero.


<<t4-q5, include=TRUE>>=

fit.null <- multinom(food ~ 1, data, weights = count)
anova(fit.null, fit, test="Chisq")
@


Again, the testing distribution used is the chi-squared distribution and in this case we are testing with 20 degrees of freedom. From the test output, we can see that the p-value for this test is 6.723e-07. This indicates a strong evidence against the null hypothesis, and at the significance level $\alpha = 0.05$, we can reject the null hypothesis that the variables $lake$, $size$ and $sex$ are jointly not significant in explaining the primary food choice of alligators.


\end{document}
