%% Do not edit preable unless you know what you are doing.
\documentclass{article}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{dsfont}
\usepackage{indentfirst}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{amsmath}

% No indents in whole document
\setlength\parindent{0pt}

% Gaps between item is itemize
\setitemize{itemsep=-1em,topsep=0.5em,parsep=0em,partopsep=0pt}

% Margins of the document
\geometry{verbose,tmargin=2.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}

% Header and footer for all pages
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\rfoot{Page \thepage}

% Header and footer for first page
\fancypagestyle{plain}{%
  \renewcommand{\headrulewidth}{0pt}%
  \fancyhf{}%
  \rhead{ETH Zurich}
  \lhead{Applied Generalized Linear Models \\ Spring Semester 2020}
  \rfoot{Page \thepage}
}

% Global Settings of R-code
<<setup, include=FALSE, cache=FALSE>>=
# Install these packages if you don't have them already
# install.packages("knitr","dplyr","ggplot2","multcomp")
library(dplyr)
library(multcomp)
library(ggplot2)
library(tinytex)
# set global chunk options
opts_chunk$set(fig.path='figures/plot-', fig.align='center', fig.show='hold')
options(formatR.arrow=TRUE,width=90)
@

% Mathematics Operators for Ease of Notation
\DeclareMathOperator{\SSR}{SSR}
\DeclareMathOperator{\SSReg}{SSReg}
\DeclareMathOperator{\SST}{SST}

% Bad Box settings
\hbadness=10000
\vbadness=\maxdimen
\vfuzz=30pt
\hfuzz=30pt


%% ---------- BEGIN DOCUMENT -----------------------------------------------
\begin{document}
%\SweaveOpts{concordance=TRUE}

%% Tiltle
\title{Assignment 2}
\author{Milan Kuzmanovic, Mark McMahon \\ Martin Kotuliak, Jakub Polak}
\date{\today}
\maketitle

\section*{Task 1}

The following table contains the estimates of a logistic regression model.

<<t1-comps, include=FALSE>>=
x1 <- -0.868
z_x1 <- -2.365
x1_se <- x1 / z_x1
or_x1 <- exp(x1)



x2 <- 2.404
or_x2 <- exp(x2)
x2_se <- 0.601

l_ci_x2 <- exp(x2-qnorm(0.975)*x2_se)
h_ci_x2 <- exp(x2+qnorm(0.975)*x2_se)

l_ci_x3 <- 0.010
h_ci_x3 <- 0.074

x3 <- (log(l_ci_x3) + log(h_ci_x3)) / 2
x3_se <- (- log(l_ci_x3) + log(h_ci_x3)) /(2*qnorm(0.975))
z_x3 <- x3/x3_se

or_x3 <- exp(x3)
@

\begin{center}
\begin{tabular}{ l r r r r r r r}
 \hline
            & & & & & & \multicolumn{2}{c}{95\% C.I. for OR}\\
 \hline
            & Est.            & s.e.              & z         & p-value & OR & lower & higher\\
 \hline
 $X_1$      & -0.868 & {\bf \Sexpr{round(x1_se, 3)}}  & -2.365     & 0.018 & {\bf \Sexpr{round(or_x1, 3)}} & 0.205 & 0.865 \\
 $X_2$      & 2.404  & 0.601 & 4.000     & <0.001 & {\bf \Sexpr{round(or_x2, 3)}} & {\bf \Sexpr{round(l_ci_x2, 3)}} & {\bf \Sexpr{round(h_ci_x2, 3)}} \\
 $X_3$      & {\bf \Sexpr{round(x3, 3)}}  & {\bf \Sexpr{round(x3_se, 3)}} & {\bf \Sexpr{round(z_x3, 3)}}    & <0.001 & {\bf \Sexpr{round(or_x3, 3)}} & 0.010 & 0.074 \\
 \hline
\end{tabular}
\end{center}

Fill in the missing information
(Please report formulas and computation.)
\begin{align*}
\text{s.e.}_{X_1}=\frac{X_1}{z_{X_1}}=\frac{\Sexpr{round(x1, 3)}}{\Sexpr{round(z_x1, 3)}}=\Sexpr{round(x1_se, 3)}\\
\text{OR}_{X_1}=e^{X_1}=e^{\Sexpr{round(x1, 3)}}=\Sexpr{round(or_x1, 3)}\\
\text{OR}_{X_2}=e^{X_2}=e^{\Sexpr{round(x2, 3)}}=\Sexpr{round(or_x2, 3)}\\
\text{lower}_{X_2}=e^{X_2 - z_{0.975} \times \text{s.e.}_{X_2}}=e^{\Sexpr{round(x2, 3)} - \Sexpr{round(qnorm(0.975), 3)}\times \Sexpr{round(x2_se, 3)}}=\Sexpr{round(l_ci_x2, 3)}\\
\text{lower}_{X_2}=e^{X_2 + z_{0.975} \times \text{s.e.}_{X_2}}=e^{\Sexpr{round(x2, 3)} + \Sexpr{round(qnorm(0.975), 3)}\times \Sexpr{round(x2_se, 3)}}=\Sexpr{round(h_ci_x2, 3)}\\
X_3=\frac{\ln \text{lower}_{X_3}+ \ln \text{higher}_{X_3}}{2} = \frac{\Sexpr{round(l_ci_x3, 3)}+ \ln \Sexpr{round(h_ci_x3, 3)}}{2} = \Sexpr{round(x3, 3)}\\
\text{s.e.}_{X_3}=\frac{- \ln \text{lower}_{X_3}+ \ln \text{higher}_{X_3}}{2 \times z_{0.975}} = \frac{- \Sexpr{round(l_ci_x3, 3)}+ \ln \Sexpr{round(h_ci_x3, 3)}}{2 \times \Sexpr{round(qnorm(0.975), 3)}} = \Sexpr{round(x3_se, 3)}\\
\text{z}_{X_3}=\frac{X_3}{\text{s.e.}_{X_3}}=\frac{\Sexpr{round(x3, 3)}}{\Sexpr{round(x3_se, 3)}}=\Sexpr{round(z_x3, 3)}\\
\text{OR}_{X_3}=e^{X_3}=e^{\Sexpr{round(x3, 3)}}=\Sexpr{round(or_x3, 3)}\\
\end{align*}

\section*{Task 2}

During the lecture, we have considered three systems of hypotheses for the parameters of the MNRM:
\begin{enumerate}
\item $H_0 : \beta_{jm} = 0 \quad vs. \quad H_1: \beta_{jm} \neq 0$
\item $H_0 : \beta_{j1} = ... = \beta_{j(M-1)} = 0 \quad vs. \quad H_1: at \ least \ one \ \beta_{jm} \neq 0, \ \forall m$
\item $H_0 : \beta_{j1} = ... = \beta_{j(M-1)} = 0 \quad vs. \quad H_1: at \ least \ one \ \beta_{jm} \neq 0, \ \forall m, j$
\end{enumerate}
Could you specify another pair of hypotheses $H_0$ and $H_1$ for the parameters of the MNRM that we might want to test? Justify your answer. \\

Firstly, it is important to explain which hypotheses can be tested, i.e. for which hypotheses is there a procedure to define a valid (asymptotic) test. The above  hypotheses are tested either based on the asymptotic normality of the maximum likelihood estimator, or on the likelihood ratio statistic whose logarithm is asymptotically Chi-square distributed. It is important to notice that we can extend the hypothesis test $H_0 : \beta_{jm} = 0 \quad vs. \quad H_1: \beta_{jm} \neq 0$ that is based on the asymptotic normality of the ML estimator to a joint test for any set of parameters. We can do this because the ML estimator is jointly normally distributed, so any subset of the set of all parameters is also jointly normally distributed, which means that by taking the quadratic form of the estimator minus hypothesized value and covariance matrix estimated by the inverse of the Fisher information matrix, we can define a Chi-square test statistic for any null hypotheses that an arbitrary subset of parameters is simultaneously equal to some specific hypothesized values (generally to zero). Therefore, we can in general construct a test for any joint hypothesis on the parameters. Now comes the question of which hypotheses might be of interest, i.e. which questions we might want an answer to? We propose several possible questions of interest defined by the below hypotheses:
\\
\begin{enumerate}
\item $H_0 : \beta_{jm} = c \quad vs. \quad H_1: \beta_{jm} \neq c$ where $c \in \mathbb{R} $ is some constant. Here we have the same logic of the test as for $c = 0$. In fact, this is just a generalization that is useful if we want to answer more specific questions about the parameters. We might be interested in testing not just for the existence of a significant effect ($c = 0$ case), but also about specific magnitude of the effect (e.g. $c = 1$).
\item $H_0 : \beta_{jm} = 0 \quad vs. \quad H_1: \beta_{jm} > 0$. We can change the alternative hypothesis in case we want to test whether there is a positive effect. Here, we assume that the parameter can't be negative, only zero under the null hypothesis or positive under the alternative hypothesis. This version of the test with different, so-called one-sided alternative, gives us more power in detecting positive (or negative in case $H_1: \beta_{jm} < 0$) effects because we assume that the effect is either zero or positive, so the rejection region for given significance $\alpha$ is two times larger on the positive (or negative) side compared to the case with the two-sided alternative.
\item $H_0 : \beta_{j1} = ... = \beta_{j(M-1)} = c \quad vs. \quad H_1: at \ least \ one \ \beta_{jm} \neq c, \ \forall m$, where $c \in \mathbb{R} $ is some constant. This is a variation of the second hypothesis test that was introduced in the lecture. Instead of testing if all of the parameters associated with variable j are zero, which would mean that variable j has no significant influence on the outcome variable, we can test that all of the parameters associated with variable j have the same effect $c \in \mathbb{R} $, on the odds relative to the reference group M. For example, having three groups, and group 3 as the reference, we might be interested if the increase in variable j results in the same change in the odds for group 1 relative to group 3 and for group 2 relative to group 3.
\item $H_0 : \beta_{1m} = ... = \beta_{pm} = 0 \quad vs. \quad H_1: at \ least \ one \ \beta_{jm} \neq 0, \ \forall j$. This hypothesis test would test if the variables in the model have any influence on the odds between group m and reference group M. We might be interested in this question if we suspect that for a specific group m the variables in the model don't affect the odds of m happening relative to M happening.
\item $H_0 : \beta_{j_{1}m} - \beta_{j_{2}m} =  0 \quad vs. \quad H_1: \beta_{j_{1}m} - \beta_{j_{2}m} \neq 0$ This hypothesis test would test if the effect of the variable $j_1$ and the variable $j_2$ on the odds of group m relative to reference group M is the same.
\end{enumerate}

There are many more possibilities for hypotheses tests. So, if we have a question of interest we can define a hypothesis test for that question, and likelihood theory allows us to construct asymptotic pivots for testing all sorts of hypotheses and thus answering all sorts of questions of interest.


\section*{Task 4}
\textbf{Q1. Write down the formulation of the MNRM assuming “fish” as reference category}\\

The general form can be written as:

\begin{multline*}
logit[\pi_m(x)] = log[\frac{\pi_m(x)}{\pi_M(x)}] = \beta_{0m} + \beta_{1m}X_{lakeHancock} + \beta_{2m}X_{lakeOklawaha}\\ + \beta_{3m}X_{lakeTrafford} + \beta_{4m}X_{sexMale} +\beta_{5m}X_{sizeSmall}
\end{multline*}

Where m $\in$ \{invertebrate, reptile, bird, other\} and M = fish. \\

For example, using reptile we have:

\begin{multline*}
logit[\pi_{reptile}(x)] = log[\frac{\pi_{reptile}(x)}{\pi_{fish}(x)}] = \beta_{0,reptile} + \beta_{1,reptile}X_{lakeHancock} + \beta_{2,reptile}X_{lakeOklawaha}\\ + \beta_{3,reptile}X_{lakeTrafford} + \beta_{4,reptile}X_{sexMale} +\beta_{5,reptile}X_{sizeSmall}
\end{multline*} \\

\textbf{Q2. Estimate the model using R. Interpret the parameters of the log-odds of reptile vs fish}\\
<<t4-q2_data, include=FALSE>>=
library(nnet)
library(tidyverse)

data <- read.csv("alligator.csv")
data$food <- relevel(data$food, "fish")
@

<<t4-q2, include=TRUE>>=
fit <- multinom(food ~ lake + sex + size, data, weights = count)

@

<<t4-table, include=FALSE>>=
output <- summary(fit)
z <- output$coefficients/output$standard.errors
p <- (1 - pnorm(abs(z), 0, 1))*2
resbird <-cbind(output$coefficients[1,],output$standard.errors[1,],z[1,],p[1,],exp(output$coefficients[1,]))
resinvert <- cbind(output$coefficients[2,],output$standard.errors[2,],z[2,],p[2,],exp(output$coefficients[2,]))
resother <- cbind(output$coefficients[3,],output$standard.errors[3,],z[3,],p[3,],exp(output$coefficients[3,]))
resreptile <- cbind(output$coefficients[4,],output$standard.errors[4,],z[4,],p[4,],exp(output$coefficients[4,]))

summary <- rbind(resbird, resinvert, resother, resreptile)
summary <- round(summary,digits=3)
colnames(summary) <- c("Est.","Std. Errors","z stat","p value","RRR")

summary <- data.frame(logit=c(rep("Bird vs. Fish",6),rep("Invertebrate vs. Fish",6),rep("Other vs. Fish",6),rep("Reptile vs. Fish",6)),
                  param=rep(rownames(resbird),4) , summary)
rownames(summary) <- NULL
@

<<t4-q2_summary, include=TRUE>>=

print(summary)

@


Above is the table to estimated parameters for the MNRM, using fish as the reference category. Below is a subset of this table with just the parameters for reptile vs fish, and their respective statistics:\\


<<t4-q2_summary_subset, include=TRUE>>=
print(summary[summary[,"logit"]=="Reptile vs. Fish",])
@

Now we can interpret these:


$\beta_{0,reptile}$ = -2.859: This indicates that the odds for the food of choice of a large female alligator from lake George to be a reptile is expected to change by a factor of $e^{-2.859}$ when compared to the odds of the same alligator's food choice being fish.\\


$\beta_{1,reptile}$ = 1.129: This indicates that the odds for reptile relative to fish is expected to change by a factor of $e^{1.129}$ if the lake variable is equal to Lake Hancock, while controlling for all other variables in the model. The p-value for this level is given as 0.841, but this should not lead us to belief to think that this whole variable is not significant as this is just one level in the categorical variable. We would need to do a global test for significance by removing this variable from a model fit and comparing the two models to get a reading on the significance or otherwise of this variable. The same can be said for the remaining two levels of this lake variable below.\\


$\beta_{2,reptile}$ = 2.530: This indicates that the odds for reptile relative to fish is expected to change by a factor of $e^{2.530}$ if the lake variable is equal to Lake Oklawaha, while controlling for all other variables in the model \\


$\beta_{3,reptile}$ = 3.061: This indicates that the odds for reptile relative to fish is expected to change by a factor of $e^{3.061}$ if the lake variable is equal to Lake Trafford, while controlling for all other variables in the model \\

$\beta_{4,reptile}$ = -0.628: This indicates that the odds for reptile relative to fish is expected to change by a factor of $e^{-0.628}$ if the sex of the alligator is male, while controlling for all other variables in the model. The p-value for this variable can be read directly as 0.360 which, if using the usual 5\% level of signifcance would lead to us failing to reject the hypothesis that the value of this parameter is equal to zero, given all the other variables in the model.  \\

$\beta_{5,reptile}$ = -0.557: This indicates that the odds for reptile relative to fish is expected to change by a factor of $e^{-0.557}$ if alligator is classes as small in size, while controlling for all other variables in the model. Again, this p-value can be read directly as 0.389 and again, this would lead to us failing to reject the hypothesis that the value of this parameter is equal to zero, given all the opther variables in the model.\\



\textbf{Q3. What is the odds ratio of bird versus other for an alligator of small size relative to an alligator of large size?  Interpret this odds ratio}\\

The odds ratio here can be given by:

\[OR = \frac{\frac{P(Y=bird | alligator=small)}{P(Y=bird | alligator = large)}}{\frac{P(Y=other | alligator=small)}{P(Y=other | alligator = large)}} = \frac{e^{\beta_{5, bird}}}{e^{\beta_{5, other}}} = e^{\beta_{5, bird} - \beta_{5, other}} = e^{-0.730 - 0.291} = e^{-1.021} = 0.360\]

This signifies that the odds for an alligator's food choice to be bird relative to it being other is expected to change by a factor of 0.360 when the alligator is noted to be of size small instead of large. \\

\textbf{Q4. Test the global significance of the variable lake.}\\

<<t4-q4, include=TRUE>>=

fit2 <- update(fit, ~ . - lake)
anova(fit, fit2, test="Chisq")
@

Here we use the update() function to remove the `lake' variable from the model, and we then produce an anova table to compare the fit of the two models. We are testing the following:

\[H_0: \beta_{1,m} = \beta_{2,m} = \beta_{3,m} = 0 ; \forall m \in \{invertebrate, reptile, bird, other\}\]

versus

\[H_0: \beta_{jm} \neq 0 ; \text{ for at least one combination of } j=1, 2, 3 and m \in \{invertebrate, reptile, bird, other\}\]

In words, we are testing the null hypothesis that the beta values for each level of the categorical variable `lake' is equal to zero for each of the Y-values, versus the alternative that at least one of the beta values is not equal to zero.\\


The testing distribution that is used is the chi-squared distribution and in this case we are testing with 12 degrees of freedom. The test statistic has a value of 50.318, which leads to a p-value of 1.228e-06. This signifies strong  evidence against the null hypothesis, and therefore we can reject the null hypothesis that the lake variable is not significant in this model set-up. \\

\textbf{Q5. Test the fit of the model}\\
Here we fit a model with just the intercept and compare the deviance of the full model with the deviance of this intercept-only model. The hypotheses we are testing is:

\[H_0: \beta_{jm} = 0 ; \forall j=1,...,6; m \in \{invertebrate, reptile, bird, other\}\]

and

\[H_0: \beta_{jm} \neq 0 ; \text{for at least one} j=1,...,6; m \in \{invertebrate, reptile, bird, other\}\]

In words, we are testing the null hypothesis that the beta values for variable in the dataset is equal to zero for each of the Y-values, versus the alternative that at least one of the beta values is not equal to zero.


<<t4-q5, include=TRUE>>=

fit.null <- multinom(food ~ 1, data, weights = count)
anova(fit.null, fit, test="Chisq")
@


Again, the testing distribution used is the chi-squared distribution and in this case we are testing with 20 degrees of freedom. From this, we can see that the p-value for this test is 6.723e-07. Assuming we are testing at the 5\% significance level, then we can again reject the null hypothesis, in this case the null hypothesis that all beta-values are equal to zero.


\end{document}
