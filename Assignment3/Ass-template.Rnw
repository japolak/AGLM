%% LyX 2.2.1 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you  know what you are doing.
\documentclass{article}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{fancyvrb}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{dsfont}

% Margins of the document
\geometry{verbose,tmargin=2.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}

% Header and footer for all pages
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\rfoot{Page \thepage}

% Header and footer for first page
\fancypagestyle{plain}{%
  \renewcommand{\headrulewidth}{0pt}%
  \fancyhf{}%
  \rhead{ETH Zurich}
  \lhead{Applied Generalized Linear Models \\ Spring Semester 2020}
  \rfoot{Page \thepage}
}

%% Global Settings
<<setup, include=FALSE, cache=FALSE>>=
library(knitr)
# set global chunk options
opts_chunk$set(fig.path='figures/plot-', fig.align='center', fig.show='hold')
options(formatR.arrow=TRUE,width=90)
@

%% ---------- BEGIN DOCUMENT -----------------------------------------------
\begin{document}

%% Tiltle
\title{Assignment 3}
\author{Milan Kuzmanovic, Mark McMahon \\ Martin Kotuliak, Jakub Polak}
\date{\today}

\maketitle

\section*{Task 1}

\textbf{(1) Import the data. Recode the variable race so that all the categories that have a frequency lower than 100 are combined into the category "other". Recode missing values in the dataset into NA.}
<<t1-setup, include=TRUE, echo=TRUE >>=
library(MASS)
freq <- read.csv("frequency.csv")

# Recoding "0" as "NA"
for (i in 1:nrow(freq)) {
  for (j in 1:ncol(freq)) {
    freq[i,j] <- ifelse(freq[i,j] == 0, NA, freq[i,j])
  }
}

# Recoding variable race
low <- as.numeric(names(table(freq$race)[which(table(freq$race) < 100)]))
for (i in 1:nrow(freq)) {
  freq$race[i] <- ifelse(freq$race[i] %in% low, 17, freq$race[i])
}

# Creating factors from nominal and ordinal variables
freq$Q2 <- factor(freq$Q2,
                  levels = c("1","2","3","4","5"),
                  labels = c("never", "rarely", "occasionally", "often", "always"))
freq$education <- factor(freq$education,
                         levels = c("1","2","3","4"),
                         labels = c("less than high school", "high school",
                                    "university degree", "graduate degree"))
freq$urban <- factor(freq$urban,
                     levels = c("1","2","3"),
                     labels = c("rural", "suburban", "urban"))
freq$gender <- factor(freq$gender,
                      levels = c("1","2","3"),
                      labels = c("male", "female", "other"))
freq$race <- factor(freq$race,
                    levels = c("11","16","17"),
                    labels = c("asian", "white","other"))
freq$married <- factor(freq$married,
                       levels = c("1","2","3"),
                       labels = c("never married","married","divorced"))
@


\textbf{(2) Consider the OLRM intercept model having Q2 as the dependent variable (i.e. the OLRM without explanatory variables). Write down the formula of the model and compute the estimate of the intercept parameters without estimating the model.}\\
\\
The general form can be written as:
\begin{align*}
log[\frac{P(Y \leq m)}{P(Y > m)}] = \beta_{0m}
\end{align*}
Where m $\in$ \{never, rarely, occasionally, often\}, so this is a model with just 4 intecept parameters. More specifically, the model is given by the following 4 equations:
\begin{align*}
log[\frac{P(Y \leq "never")}{P(Y > "never")}] = \beta_{0never}
\end{align*}
representing the log-odds of "never" vs. "rarely", "occasionally", "often" and "always".
\begin{align*}
log[\frac{P(Y \leq "rarely")}{P(Y > "rarely")}] = \beta_{0rarely}
\end{align*}
representing the log-odds of "never" and "rarely" vs. "occasionally", "often" and "always".
\begin{align*}
log[\frac{P(Y \leq "occasionally")}{P(Y > "occasionally")}] = \beta_{0occasionally}
\end{align*}
representing the log-odds of "never", "rarely" and "occasionally" vs. "often" and "always".
\begin{align*}
log[\frac{P(Y \leq "often")}{P(Y > "often")}] = \beta_{0often}
\end{align*}
representing the log-odds of "never", "rarely", "occasionally" and "often" vs. "always".\\
\\
The intercepts are easily obtained without estimating the model because in the absence of explanatory variables the odds are equal to the empirical odds obtained by counting the number of instances of two events. So, the intercept parameters are equal to log of empirical odds:
<<t1-intercepts, include=TRUE, echo=TRUE >>=
beta_never <- log(sum(na.omit(as.numeric(freq$Q2) <= 1))/sum(na.omit(as.numeric(freq$Q2) > 1)))
beta_rarely <- log(sum(na.omit(as.numeric(freq$Q2) <= 2))/sum(na.omit(as.numeric(freq$Q2) > 2)))
beta_occasionally <- log(sum(na.omit(as.numeric(freq$Q2) <= 3))/sum(na.omit(as.numeric(freq$Q2) > 3)))
beta_often <- log(sum(na.omit(as.numeric(freq$Q2) <= 4))/sum(na.omit(as.numeric(freq$Q2) > 4)))
@
We have:\\
\begin{align*}
 \beta_{0never} = \Sexpr{round(beta_never, 3)}\\
 \beta_{0rarely} = \Sexpr{round(beta_rarely, 3)}\\
 \beta_{0occasionallt} = \Sexpr{round(beta_occasionally, 3)}\\
 \beta_{0often} = \Sexpr{round(beta_often, 3)}\\
\end{align*}\\
\\
\textbf{(3) Use the backward selection procedure to select the best fitting OLRM. The code we used in the practical session on OLRM returns an error. Explain why the error occurs and correct the code so that you can select the best fitting model.}
<<t1-beckward-selection, include=TRUE, echo=TRUE >>=
mod.full <- polr(Q2~., data = na.omit(freq))
mod.fin <- step(mod.full, direction = "backward")
@
The reason for the error can be read off in the error message. The $polr()$ function is not equipped to handle missing values. There are two main ways of treating missing values. One is discarding the rows that contain missing values, and the other is data imputation. If there are not many data points with missing values then removing the rows with missing values is a reasonable practice. However, if the a lot of data goes to waste by removing rows with some missing values, then imputation might be better. We first try the first method as it is easier to implement and does not require selcting the imputation procedure.
<<t1-backward, include=TRUE, echo=TRUE >>=
freq2 <- na.omit(freq)
dim(freq)
dim(freq2)
@
We see that we lose around $6.5\%$ of the data by removing all rows with any missing values, which should not disturb the fit as it is only a small fraction of the data, and hence we choose to remove missing values instead of impute them.
<<t1-beckward2, include=TRUE, echo=TRUE >>=
mod.full <- polr(Q2~., data = freq2)
mod.fin <- step(mod.full, direction = "backward")
@
We use the $step()$ function which selects the best model based on the AIC criterion. The best model in terms of AIC is the one with explanatory variables age, gender, eduaction and race. \\
\\
\textbf{(4) Consider the best fitting OLRM returned by the backward selection procedure. Test its goodness of fit using an adequate test.}\\
\\
The formula of the best fitting model $mod.fin$ is the following:
\begin{align*}
log[\frac{P(Y \leq m)}{P(Y > m)}] = \beta_{0m} + \beta_{1}D_{highschool} + \beta_{2}D_{university} + \beta_{3}D_{graduate} + \\
\beta_{4}D_{female} + \beta_{5}D_{gender:other} +  \beta_{6}X_{age} + \beta_{7}D_{white} + \beta_{8}D_{race:other}
\end{align*}
In order to test the fit of the best fitting model by selction via AIC, we need to perform a Likelihood ratio test and compare the deviance between model with just an intercept and the the best fitting model. The LRT has a test statistic
\begin{align*}
G = Dev(mod.empty) - Dev(mod.fin)
\end{align*}
which has asymptotic $\chi_4^2$ distribution under the null hypothesis
\begin{align*}
H_0: \beta_{1} = \beta_{2} = \beta_{3} = \beta_{4} = \beta_{5} = \beta_{6} = \beta_{7} = \beta_{8} = 0
\end{align*}
since the best fitting model "mod.fin" has 4 explanatory variables. We are testing the above null hypothesis versus the alternative hypothesis:
\begin{align*}
At \: least \: one \: \beta_j \neq 0, \: for \: j = 1,2,3,4,5,6,7,8
\end{align*}
The code below performs the test:
<<t1-model-fit, include=TRUE, echo=TRUE >>=
mod.empty <- polr(Q2~1, data = freq2)
anova(mod.empty, mod.fin, test = "Chisq")
@
We have the value of the G statistic equal to $\Sexpr{round(anova(mod.empty, mod.fin, test = "Chisq")$`LR stat.`[2], 3)}$ and the corresponding p-value equal to $\Sexpr{round(anova(mod.empty, mod.fin, test = "Chisq")$`Pr(Chi)`[2])}$, which is close to zero and much lower than the general significance level 0.05. Hence, at significance level $\alpha = 5\%$, we reject the null hypothesis and conclude that the best model selected by AIC $mod.fin$ has a better fit than the model with just an intecept $mod.empty$.\\
\\
\textbf{(5) Interpret the model coefficients.}\\
\\
First we need to address the significance of the coefficient estimates, since only those that are statistically significantly different from zero deserve an interpretation of the value of the estimate. The fact that this model is the best model by AIC criterion is a strong indication that the explanatory variables are all significant. In fact, because the explanatory variables are multi-level factors, we have two notions of significance, i.e. one related to significance of the factor as a whole, and the other related to significance of the differences between the factor levels of the same explanatory variable. The significance of factor explanatory variables as a whole can be examined with $drop1()$ function and this one determines whether an explanatory variable as a whole is significant, i.e. is any of the coefficients related to this variable significantly different from zero. The significance of the individual coefficient estimates is examined with $summary()$ function and it relates to significance of the difference between reference category of explanatory factor and its other levels. Note that if explanatory factor as a whole is significant, we have to include all of its levels in the model, regardless of whether some levels are not significantly different from the reference level.
<<t1-significance, include=TRUE, echo=TRUE >>=
drop1(mod.fin, test = "Chisq")
out <- summary(mod.fin)
out$coefficients[1:8,c(1,3)] <- -out$coefficients[1:8,c(1,3)]
p <- (1 - pnorm(abs(out$coefficients[,3]), 0, 1))*2
OR <- c(exp(out$coefficients[,1]))
data.frame(round(cbind(out$coefficients, pvalue = p, OR = OR),5))
@
Another thing to keep in mind is that the estimates above are modified so that they fit $\beta$-parameterization as it is specified in the model formula. $polr()$ function uses different parameterization and hence we need to invert the sign to obtain correct $\beta$ estimates. Note that we omit the interpretation of the intercepts as with OLRM we are generally only interested in the parameters related to explanatory variables. Also, here we put the interpretation for the odds of "never" vs. "rarely", "occasionally", "often" and "always", however, due to the parallel slopes assumption, equivalent interpretation holds also for the odds of "never" and "rarely" vs. "occasionally", "often" and "always", the odds of "never", "rarely" and "occasionally" vs. "often" and "always", and the odds of "never", "rarely", "occasionally" and "often" vs. "always".The parameters are interpreted as follows:
\begin{itemize}

\item $\beta_1 = \Sexpr{round(out$coefficients[1,1],3)}$: The parameter $\beta_{1}$ represents the difference between the log-odds (or the logit) of "never" vs. "rarely", "occasionally", "often" and "always" for a person with high school eduaction compared to a person with less than high school education, while holding all other variables in the model constant, i.e. holding age, gender and race constant. In a more intuitive inerpretation, the odds of "never" vs. "rarely", "occasionally", "often" and "always" change by a factor of $e^{\beta_1}$, or by $(e^{\beta_1} - 1)\cdot100\%$, when going from less than high school education to high school education, while keeping all other variables constant. The estimate for parameter $\beta_{1}$ is $\Sexpr{round(out$coefficients[1,1],3)}$, which means that the data shows that the odds of "never" vs. "rarely", "occasionally", "often" and "always" for a person with high school education are $\Sexpr{round(exp(out$coefficients[1,1]),3)}$ times the same odds for a person with less than high school education, i.e. $\Sexpr{(round(exp(out$coefficients[1,1]),3) -1)*100}$$\%$ lower odds for a person with high school education than for a person with less than high school education, keeping age, gender and race constant. This would indicate that getting high school education raises awareness about our actions affecting the environment. However, we must note that the p-value for this estimate is $\Sexpr{round(p[1],3)}$ meaning that there is no significant difference between high school eduaction and less than high school education in terms of association with environmental awareness, but since there are significant differences in the levels of factor education as a whole, all levels deserve an interpretation.

\item $\beta_2 = \Sexpr{round(out$coefficients[2,1],3)}$: The parameter $\beta_{2}$ represents the difference between the log-odds (or the logit) of "never" vs. "rarely", "occasionally", "often" and "always" for a person with university degree eduaction compared to a person with less than high school education, while holding all other variables in the model constant, i.e. holding age, gender and race constant. In a more intuitive inerpretation, the odds of "never" vs. "rarely", "occasionally", "often" and "always" change by a factor of $e^{\beta_2}$, or by $(e^{\beta_2} - 1)\cdot100\%$, when going from less than high school education to university degree education, while keeping all other variables constant. The estimate for parameter $\beta_{2}$ is $\Sexpr{round(out$coefficients[2,1],3)}$, which means that the data shows that the odds of "never" vs. "rarely", "occasionally", "often" and "always" for a person with university degree education are $\Sexpr{round(exp(out$coefficients[2,1]),3)}$ times the same odds for a person with less than high school education, i.e. $\Sexpr{(round(exp(out$coefficients[2,1]),3) -1)*100}$$\%$ lower odds for a person with university degree education than for a person with less than high school education, keeping age, gender and race constant. This would indicate that getting university degree education raises awareness about our actions affecting the environment. However, we must note that the p-value for this estimate is $\Sexpr{round(p[2],3)}$ meaning that there is no significant difference between university degree eduaction and less than high school education in terms of association with environmental awareness, but since there are significant differences in the levels of factor education as a whole, all levels deserve an interpretation.

\item $\beta_3 = \Sexpr{round(out$coefficients[3,1],3)}$: The parameter $\beta_{3}$ represents the difference between the log-odds (or the logit) of "never" vs. "rarely", "occasionally", "often" and "always" for a person with graduate degree eduaction compared to a person with less than high school education, while holding all other variables in the model constant, i.e. holding age, gender and race constant. In a more intuitive inerpretation, the odds of "never" vs. "rarely", "occasionally", "often" and "always" change by a factor of $e^{\beta_3}$, or by $(e^{\beta_3} - 1)\cdot100\%$, when going from less than high school education to graduate degree education, while keeping all other variables constant. The estimate for parameter $\beta_{3}$ is $\Sexpr{round(out$coefficients[3,1],3)}$, which means that the data shows that the odds of "never" vs. "rarely", "occasionally", "often" and "always" for a person with graduate degree education are $\Sexpr{round(exp(out$coefficients[3,1]),3)}$ times the same odds for a person with less than high school education, i.e. $\Sexpr{(round(exp(out$coefficients[3,1]),3) -1)*100}$$\%$ lower odds for a person with graduate degree education than for a person with less than high school education, keeping age, gender and race constant. This would indicate that getting graduate degree education raises awareness about our actions affecting the environment, and the p-value for this estimate is $\Sexpr{round(p[3],3)}$ meaning that the difference between graduate degree eduaction and less than high school education in terms of association with environmental awareness is significant.

\item $\beta_4 = \Sexpr{round(out$coefficients[4,1],3)}$: The parameter $\beta_{4}$ represents the difference between the log-odds (or the logit) of "never" vs. "rarely", "occasionally", "often" and "always" for women compared to men, while holding all other variables in the model constant, i.e. holding age, education and race constant. In a more intuitive inerpretation, the odds of "never" vs. "rarely", "occasionally", "often" and "always" change by a factor of $e^{\beta_4}$, or by $(e^{\beta_4} - 1)\cdot100\%$, when going from male to female, while keeping all other variables constant. The estimate for parameter $\beta_{4}$ is $\Sexpr{round(out$coefficients[4,1],3)}$, which means that the data shows that the odds of "never" vs. "rarely", "occasionally", "often" and "always" for women are $\Sexpr{round(exp(out$coefficients[4,1]),3)}$ times the same odds for men, i.e. $\Sexpr{(round(exp(out$coefficients[4,1]),3) -1)*100}$$\%$ lower odds for women than for men, keeping age, education and race constant. This would indicate that women have more awareness about how their actions affect the environment compared to men, and the p-value for this estimate is $\Sexpr{round(p[4],3)}$ meaning that the difference between women and men in terms of association with environmental awareness is significant.

\item $\beta_5 = \Sexpr{round(out$coefficients[5,1],3)}$: The parameter $\beta_{5}$ represents the difference between the log-odds (or the logit) of "never" vs. "rarely", "occasionally", "often" and "always" for gender other compared to men, while holding all other variables in the model constant, i.e. holding age, education and race constant. In a more intuitive inerpretation, the odds of "never" vs. "rarely", "occasionally", "often" and "always" change by a factor of $e^{\beta_5}$, or by $(e^{\beta_5} - 1)\cdot100\%$, when going from male to gender other, while keeping all other variables constant. The estimate for parameter $\beta_{5}$ is $\Sexpr{round(out$coefficients[5,1],3)}$, which means that the data shows that the odds of "never" vs. "rarely", "occasionally", "often" and "always" for gender other are $\Sexpr{round(exp(out$coefficients[5,1]),3)}$ times the same odds for men, i.e. $\Sexpr{(round(exp(out$coefficients[5,1]),3) -1)*100}$$\%$ lower odds for gender other than for men, keeping age, education and race constant. This would indicate that persons that identify their gender as other have more awareness about how their actions affect the environment compared to men, and the p-value for this estimate is $\Sexpr{round(p[5],3)}$ meaning that the difference between gender other and men in terms of association with environmental awareness is significant.

\item $\beta_6 = \Sexpr{round(out$coefficients[6,1],3)}$: The parameter $\beta_{6}$ represents the change in the log-odds (or the logit) of "never" vs. "rarely", "occasionally", "often" and "always" for a unit increase in age (one year increase), while holding all other variables in the model constant, i.e. holding gender, education and race constant. In a more intuitive inerpretation, the odds of "never" vs. "rarely", "occasionally", "often" and "always" change by a factor of $e^{\beta_6}$, or by $(e^{\beta_6} - 1)\cdot100\%$, when increasing age by one year, while keeping all other variables constant. The estimate for parameter $\beta_{6}$ is $\Sexpr{round(out$coefficients[6,1],3)}$, which means that the data shows that the odds of "never" vs. "rarely", "occasionally", "often" and "always" for a year older individual are $\Sexpr{round(exp(out$coefficients[6,1]),3)}$ times the same odds for a year younger individual, i.e. $\Sexpr{(round(exp(out$coefficients[6,1]),3) -1)*100}$$\%$ lower odds for a year older individual than for a year younger one, keeping gender, education and race constant. This would indicate that older persons have more awareness about how their actions affect the environment compared to younger ones, and the p-value for this estimate is $\Sexpr{round(p[6],3)}$ meaning that the difference between a year older and a year younger individual in terms of association with environmental awareness is significant.

\item $\beta_7 = \Sexpr{round(out$coefficients[7,1],3)}$: The parameter $\beta_{7}$ represents the difference between the log-odds (or the logit) of "never" vs. "rarely", "occasionally", "often" and "always" for a white person compared to an asian person, while holding all other variables in the model constant, i.e. holding age, gender and education constant. In a more intuitive inerpretation, the odds of "never" vs. "rarely", "occasionally", "often" and "always" change by a factor of $e^{\beta_7}$, or by $(e^{\beta_7} - 1)\cdot100\%$, when going from race asian to race white, while keeping all other variables constant. The estimate for parameter $\beta_{7}$ is $\Sexpr{round(out$coefficients[7,1],3)}$, which means that the data shows that the odds of "never" vs. "rarely", "occasionally", "often" and "always" for a white person are $\Sexpr{round(exp(out$coefficients[7,1]),3)}$ times the same odds for an asian person, i.e. $\Sexpr{(round(exp(out$coefficients[7,1]),3) -1)*100}$$\%$ larger odds for a white person than for an asian person, keeping age, gender and education constant. This would indicate that white people have less awareness about how their actions affect the environment compared to asian people. However, we must note that the p-value for this estimate is $\Sexpr{round(p[7],3)}$ meaning that there is no significant difference between white people and asian people in terms of association with environmental awareness, but since there are significant differences in the levels of factor race as a whole, all levels deserve an interpretation.

\item $\beta_8 = \Sexpr{round(out$coefficients[8,1],3)}$: The parameter $\beta_{8}$ represents the difference between the log-odds (or the logit) of "never" vs. "rarely", "occasionally", "often" and "always" for a race other person compared to an asian person, while holding all other variables in the model constant, i.e. holding age, gender and education constant. In a more intuitive inerpretation, the odds of "never" vs. "rarely", "occasionally", "often" and "always" change by a factor of $e^{\beta_8}$, or by $(e^{\beta_8} - 1)\cdot100\%$, when going from race asian to race oyhrt, while keeping all other variables constant. The estimate for parameter $\beta_{8}$ is $\Sexpr{round(out$coefficients[8,1],3)}$, which means that the data shows that the odds of "never" vs. "rarely", "occasionally", "often" and "always" for a race other person are $\Sexpr{round(exp(out$coefficients[8,1]),3)}$ times the same odds for an asian person, i.e. $\Sexpr{(round(exp(out$coefficients[8,1]),3) -1)*100}$$\%$ lower odds for a race other person than for an asian person, keeping age, gender and education constant. This would indicate that people of race other have more awareness about how their actions affect the environment compared to asian people. However, we must note that the p-value for this estimate is $\Sexpr{round(p[8],3)}$ meaning that there is no significant difference between race other people and asian people in terms of association with environmental awareness, but since there are significant differences in the levels of factor race as a whole, all levels deserve an interpretation.

\end{itemize}

Note that for factor race, both our estimates are not significant and we saw that there is no significant difference between asian race and white race, nor between asian race and race other. On the other hand, the race as factor is significant meaning that there are significant differences within the factor. Since the conclusions for white race and race other were opposite when compared to asian race, we could probably say that the significant difference in factor race lies in the difference between white race and race other, with race other people being significantly more aware about how their actions affect the environment compared to white people. However, we cannot make such comment without performing a statistical test for this statement, which we could do by changing the reference category of factor race from asian to white or to other.

\newpage

\section*{Task 2}

\textbf{The assumption of parallel slopes (or proportional odds) underlying the OLRM implies that the coefficients describing the relationship between the ordinal dependent variable and the explanatory variables are the same between any pair of adjacent categories. \\
\\
Develop a procedure to informal assess the assumption of parallel slopes.}\\
\\
\textbf{(1)  Describe the procedure and its logic.}\\
\\
Since we are assuming that the effect of, say education, is the same for each change from one ordered outcome category to the next, we are therefore saying that the effect of the education variable, $\beta_{education}$, is the same when changing from each category to the next. One way to informally test this is by building four binary logistic regression models (one for each `jump' in category, e.g. one model would be for the change in category between `never' and `rarely', another for the change between `rarely' and `occassional', and so on). We would build these considering just one explanatory variable at a time, and we would transform the outcome variable to be binary, e.g. for modeling the change between `never' and `rarely', we would make the outcome category 0 where it was originally `never' or `rarely', and 1 where it is `occassional', `often', or `always'. \\
\\
\textbf{(2)  Apply the developed procedure to check whether the parallel slopes assumption is matched by the data in Task 1 and the explanatory variables included in the best fitting model (Task 1, point 2)}.
\\
Consider the explanatory variable education. After we fit the four models using the binary transform outlined above, we can then plot the logit `fits' for these four models. `Fits' here refers to the $\beta_{interccept} + \beta_{<education:dummy>}$ value which can be obtained from the fitted model. Therefore, for each model we will get 4 plotted points (one for the intercept value with the reference category of education, and 3 more for the intercept value plus the beta corresponding to the next education category). If our assumption holds, we expect these 4 lines to be approximately the same in slope and approximately pairwise equidistant at each point (where each point represents a different education category) on the line.\\

Below we can see the result of this procedure used with the 4 variables that were chosen by the best fitting model, namely education, gender, race, and age. From this, we can informally deduce whether our original assumption holds. For the`age' variable it clearly holds, while for the other three variables it appears to hold too, though perhaps not as obviously as for `age' It is up to the analyst here whether to perform more rigorous tests to further check this assumption, however from this quick visual inspection it would appear that the assumption holds well enough.

<<t2-plot, include=TRUE, echo=FALSE >>=

all_coefs <- matrix(ncol = 5, nrow=0)  # model var, beta label, beta value, standard error

layout(matrix(c(1,2,3,4,5,5), ncol=2, byrow=TRUE), heights = c(2,2,0.5))

vars <- c("education", "gender", "race", "age")
q2_levels <- levels(freq2[,"Q2"])

for(v in vars){
  par(mai=rep(0.55, 4))
  p <- nlevels(freq2[,v])
  p <- ifelse(p==0, 2, p)  # age isn't a factor var so we manually set this to 2 so we have 2 columns, one for intercept one for effect of1 unit increase
  glm_coefs = matrix(ncol = p, nrow=4)

  for(i in c(1, 2, 3, 4)){
    df <- freq2  # So we don't overwrite the original data with our changes
    df[,"Q2"] = (!(df[,"Q2"] %in% q2_levels[1:i]))*1

    f = glm(as.formula(paste0("Q2~", v)), data=df, family = "binomial")
    glm_coefs[i,] <- f$coef
    coefs <- f$coef
    coefs[2:p] <- coefs[2:p] + coefs[1]

    coef_df <- data.frame(summary(f)$coefficients[, 1:2])
    coef_df <- cbind(beta_label=rownames(coef_df), coef_df)
    rownames(coef_df) <- NULL
    coef_df <- cbind(variable=v, m_thresh=i, coef_df)
    all_coefs <- rbind(all_coefs, coef_df)


  }

  coefs_plot <- glm_coefs

  if(v=="age"){
    coefs_plot[,2] = coefs_plot[,1] + (coefs_plot[,2] * max(df[,"age"]))
  }
  else{
    coefs_plot[, 2:p] <- coefs_plot[, 2:p] + coefs_plot[, 1]
  }

  plot(coefs_plot[1,], type="b", ylim=c(min(coefs_plot)*1.1, max(coefs_plot)*1.1), lwd=2, pch=19, xlab=v, ylab="logit", col="red", xaxt="n")
  lines(coefs_plot[2,], type="b", col="blue", lwd=2, pch=19, xaxt="n")
  lines(coefs_plot[3,], type="b", col="purple", lwd=2, pch=19, xaxt="n")
  lines(coefs_plot[4,], type="b", col="green", lwd=2, pch=19, xaxt="n")

  if(v=="education"){
    labs <- c("Less than HS", "High School", "University", "Graduate")
  }
  else if(v=="gender"){
    labs <- c("Male", "Female", "Other")
  }
  else if(v=="race"){
    labs <- c("Asian", "White", "Other")
  }
  else{
    labs <- c(0, max(df[,"age"]))
  }
  axis(1, at=1:p, labels=labs)
}


par(mar=c(0,0,0,0))
plot(1, type = "n", axes=FALSE, xlab="", ylab="")
plot_colours <- c("red", "blue", "purple", "green")
legend(x = "bottom",inset = 0,cex=1,
       legend = c("never vs rarely, occasionally, often, always", "never, rarely vs occasionally, often, always",
                  "never, rarely, occasionally vs often, always", "never, rarely, occasionally, often vs always"),
       col=plot_colours, lwd=2, pch=19, ncol=2)


@

\newpage

Below we have a table of coefficients which we can also use for informally testing this assumption. The coefficients for the full model that was chosen in task 1, part 3 are shown, along with the coefficients from the individual binary logistic regression models which were described above. Here, `m<=1' indicates the model where the outcome variable Y was transformed to be 0 if Y was 1, and 1 otherwise. Similar for `m<=2' and so on.

Again, we are checking if the beta coefficients are similar across these models. Taking standard error into account allows us to see the confidence in our estimates, and therefore gives our test some more credibility. From looking at these, we can again say that our parallel assumption can be accepted for these betas. Some values are dubious, for example the estimate for `education: high school' for the full model is 0.135, while it is -0.059 for `m<=1' - however when we take the standard errors for these estimates into account (0.173 and 0.403 respectively), we can see that perhaps this difference is significant. Having said that, for these values that are quite different it would be worth it to perform a more formal test for this assumption.

<<t2_table, echo=FALSE>>=
# Getting tables together
complete_table <- data.frame()
# Table of full model coefs
full_model_c <- data.frame(summary(mod.fin)$coefficients[,1:2])
full_model_c[, "beta_label"] <- rownames(full_model_c)
rownames(full_model_c) <- NULL
full_c <- reshape(full_model_c, varying = c("Value", "Std..Error"), v.names = "Full model",
             idvar = c("beta_label"),
             timevar = "Beta", direction="long", times = c("Estimate", "Std..Error"))
rownames(full_c) <- NULL

# Education coefs

for(v in vars){
  c_table <- all_coefs[all_coefs[,"variable"] == v, -1]

  z <- reshape(c_table, varying = c("Estimate", "Std..Error"), v.names = "Value",
          idvar = c("m_thresh", "beta_label"),
          timevar = "Beta", direction="long", times = c("Estimate", "Std..Error"))
  rownames(z) <- NULL

  label_order = paste0(v, levels(freq2[,v]))
  label_order[1] = "(Intercept)"
  z <- z[order(z$m_thresh, match(z$beta_label, label_order)), ]

  final_table <- reshape(z, idvar = c('beta_label', 'Beta'), direction = 'wide',
          timevar = 'm_thresh', sep = '_model_')

  final_table <- merge(full_c,final_table, by = c('beta_label', "Beta"))
  complete_table <- rbind(complete_table, final_table)
}

complete_table[,3:7] <- round(complete_table[,3:7], 3)
colnames(complete_table) <- c("beta_label", "Beta", "Full Model", "m<=1", "m<=2", "m<=3", "m<=4")
print(complete_table)


@

\newpage

\section*{Task 3}


<<t3-setup, include=FALSE, cache=FALSE>>=
library(ggplot2)
library(MASS)
library(reshape2)
library(tidyverse)
library(car)
library(sandwich)
library(AER)

medpar <- read.csv("medpar.csv")

medpar$hmo <- factor(medpar$hmo, levels=c("0", "1"), labels=c("private", "hmo"))
medpar$white <- factor(medpar$white, levels=c("0", "1"), labels=c("non-white", "white"))
medpar$age80 <- factor(medpar$age80, levels=c("0", "1"), labels=c("<80", ">80"))
medpar$type <- factor(medpar$type, levels=c("1", "2", "3"), labels=c("elective", "urgent", "emergency"))
@

The data set medpar.csv is an excerpt from US national Medicare inpatient hospital database. It contains 1495 observations on the following variables:\\
\\
\begin{itemize}
\item los: length of hospital stay (in days)
\item hmo: patient belongs to a Health Maintenance Organization (1), or private
pay (0)
\item white: patient identifies themselves as primarily Caucasian (1) in comparison to non-white (0)
\item age80: patient age 80 and over (1), or age < 80 (0)
\item type: a three-level explanatory variable related to the type of admission
(1 = elective, 2 = urgent, and 3 = emergency)
\end{itemize}\\
\\
We would like to investigate whether there is an association between the length of hospital stay and the other variables.\\
\\
\begin{enumerate}
\item Import the data. Based on the descriptive statistics, do you expect that there is a significant relation between \texttt{los} and \texttt{type}? Justify your answer.

<<t3-expl_anal, fig.height=3, echo=FALSE, fig.cap="\\label{fig:boxplot}Boxplot showing relationship between los and type">>=
ggplot(medpar, aes(x=type, y=los, fill=type)) + geom_boxplot()+ coord_trans(y = "log10") +stat_summary(fun.y=mean, color='black', show.legend = FALSE, geom = "point", shape=19, size=3) + theme_bw() + scale_y_continuous(breaks=c(1,5, 10, 20, 50, 100))
@

From the boxplot in \autoref{fig:boxplot} we can see that mean, median, top quartiles and top extremes are all rising with how serious the type of the visit is. On the other hand bottom quartile and bottom extreme values are on the same level between types of stay. There might be different reason for low extremes between the stays. For elective stay the short stay is expected due to nature of procedures. However for emergency, low stay might mean that a patient died.

In order to see if the means of the length of stay between the types of stay are signifficantly different, we fit a ANOVA model.

<<t3-aov>>=
mod0 <- aov(los ~ type, data=medpar)
summary(mod0)
@

P-value of \texttt{<2e-16} shows that we can reject null hypothesis that the means of types are the same.

\item Estimate a Poisson regression model with los as dependent variable and type as explanatory variable. Name this model Model 1. Interpret the parameters of the model (including the intercept).

<<t3-sim_poi>>=
mod1 <- glm(los ~ type, family="poisson", data=medpar)
summary(mod1)
@

Null hypothesis for each explanatory variable is that the factor is 0. Looking at the p-value, all are below 0.05 therefore we can reject the null hypothesis.

To get expected value $E[Y|\mathbf{x}]$ in poisson regression model:

$$
E[Y|\mathbf{x}] = \lambda= = e^{\beta_{intercept} + \beta_{urgent}x_{urgent} + \beta_{emergency}x_{emergency}}
$$

For individual type of stays, we get following predicted values for length of stay:

<<t3-predicted>>=
predict(mod1, newdata = data.frame(type=factor(c(1,2,3),
                                   levels=c("1", "2", "3"),
                                   labels=c("elective", "urgent", "emergency"))),
                                   type="response")
@

Where columns corresponds to los for elective stay, urgent stay and emergency stay respectively. We can see that length of emergency stay is the longest and elective the shortests as expected. The difference between elective and urgent is 2.37 days and the difference between urgent and emergency is 7.04. Even though the difference in factors is low, since expected value of poisson regression is a exponential function, the differences in the lenghts of stay are large.

\item Add to the model in (2) the explanatory variables age80, hmo and white. Name the resulting model Model 2. Test whether Model 2 has a better fit than Model 1.

To compare the two models we can use Likelihood Ratio Test (LRT). We define two hypotheses:

$$
H_0:\beta_{white}=\beta_{age80}=\beta_{hmo}=0 \qquad vs. \qquad H_1:\beta_{white}\neq 0 \; \text{or} \; \beta_{age80}\neq 0  \; \text{or} \; \beta_{hmo}\neq 0
$$


<<t3-comp_poi>>=
mod2 <- glm(los ~ type + white + age80 + hmo, family="poisson", data=medpar)
summary(mod2)
anova(mod1, mod2, test="Chisq")
@

At p-value of 1.86e-10 we can reject null hypothesis and conclude second model performs better. The difference of deviances of two models is greater than 95th quantile for $\chi^2_3$.

<<t3-lev, fig.width=8, fig.height=5.5, out.width="8.5in", out.width="6in", echo=FALSE, fig.cap="\\label{fig:outliers}Diagnostic plots for model1 and model2 respectively">>=
influenceIndexPlot(mod1,vars=c("Studentized", "hat", "Cook"), id=list(n=c(2)))
influenceIndexPlot(mod2,vars=c("Studentized", "hat", "Cook"), id=list(n=c(2)))
@

From diagnostic plots we can find out we have two levarage points. They might greatly influence estimates in our models.

<<t3-lev2>>=
medpar[c(1452, 1466),]
@

These are two patients which stayed in the hospital for a very long period. We will see in latter subquestion negative binomial models which accounts for data with greater variance.

\item Interpret the parameter related to the variable age80.

For patients with age greater than 80, we see shorter stay in hospitals by factor of -0.05471.

<<t3-age_anal, fig.height=3, echo=FALSE, fig.cap="\\label{fig:boxplot2}Boxplot showing relationship between los and type with respect to age group">>=
ggplot(medpar, aes(x=type:age80, y=los, fill=type)) + geom_boxplot()+ coord_trans(y = "log10") +stat_summary(fun.y=mean, color='black', show.legend = FALSE, geom = "point", shape=19, size=3) + theme_bw() + scale_y_continuous(breaks=c(1,5, 10, 20, 50, 100))
@

From \autoref{fig:boxplot2} we can conclude that the greatest difference between two age groups is for emergency stay. Older patients stay shorter time in hospitals in case of emergency. With that in mind simple explanation is the death of patients. Older patients have a higher chance of dying in the hospitals. Therefore, final length of stay will be actually shorter.

\item Test whether the equi-dispersion assumption is matched by the data. If that
is not the case:
\begin{enumerate}
\item Estimate an adequate model including all the explanatory variables. Name this model Model 3.
\item Compare the results of Model 2 and Model 3. Are they the same?
\end{enumerate}

<<t3-over, echo=FALSE>>=
with(medpar, tapply(los, type, function(x) {
  sprintf("M (Var) = %1.2f (%1.2f)", mean(x), var(x))
}))
with(medpar, tapply(los, white, function(x) {
  sprintf("M (Var) = %1.2f (%1.2f)", mean(x), var(x))
}))
with(medpar, tapply(los, age80, function(x) {
  sprintf("M (Var) = %1.2f (%1.2f)", mean(x), var(x))
}))
with(medpar, tapply(los, hmo, function(x) {
  sprintf("M (Var) = %1.2f (%1.2f)", mean(x), var(x))
}))
@

We see a case of overdispersion in our data. We use \Verb+dispersiontest()+ on the poisson-model (Model2) included int the \Verb+AER+-package to test for dispersion. Poisson regression assumes $\mathds{E}[Y|X] = \mu$ and $\mathds{V}[Y|X] = \mu$. This test assumes that $\mathds{V}[Y|X] = \mu + \alpha \times \text{trafo}(\mu)$ and tests $H_0: \alpha=0$. As we know from the negative binomial model and evidence above, we should test for over-dispersion, and thus set the $H_A$ to “greater” than 0 ($H_A: \alpha > 0$). As shown in the lecture, in negative binomial model, the variance is a quadratic function of the mean, and hence we set trafo$(\mu)=2$.

Performing the above described test:
<<t3-overdisp>>=
dispersiontest(mod2,alternative="greater",trafo=2)
@

One can see that $\alpha$ is detected as 0.6015419. We reject the null hypotheses that our data are equidispersed since p-value is <0.05, therefore, true dispersion parameter $\alpha$ is non zero. Thus we conclude that we deal with over-dispersed data and the pure poisson-model is not adequate. We can fit a negative binomial regression model which has a dispersion parameter $\theta$ (different from $\alpha$) to better fit the variance of data.

<<t3-nbm>>=
mod3 <- glm.nb(los ~ type + white + age80 + hmo, data=medpar)
summary(mod3)
@

The dispersion parameter $\theta$ is equal to 2.2458 with standard error 0.0999. Variance of the data is $\lambda + \frac{\lambda^2}{\theta}$. When comparing estimates of model2 and model3 we see that their absolute value is smaller for model3. That is because model3 does not need to compensate larger variance with larger estimates.

Morover we see that for model3, we cannot reject null hypothesis for hmo, age80 nad white parameters that they are equal to 0.

<<t3-nbm_anal, fig.width=8, fig.height=5.5, out.width="8.5in", out.width="6in", echo=FALSE, fig.cap="\\label{fig:outliers2}Diagnostic plots for negative binomial model">>=
influenceIndexPlot(mod3,vars=c("Studentized", "hat", "Cook"), id=list(n=c(2)), main=NULL)
@
Comparing Diagnostic plots \autoref{fig:outliers} and \autoref{fig:outliers} we see that we do not observe two levarege points for negative binomial model. Negative binomial model accounts for higher variance and from studentized residuals we see that fit of this model is overall better than the one of model2.
\end{enumerate}

\newpage

\section*{Task 4}

Could you think of a test for equi-dispersed data based on the comparison of two models rather than testing the over-dispersion parameter $\alpha$ of the negative binomial regression model? Define the null and the alternative hypotheses, the test statistic and the rejec- tion region. Explain the logic of the test. Apply the test to the data in Task 3.\\

As we mentioned earlier, negative binomial models assume the conditional means are not equal to the conditional variances. This inequality is captured by estimating a dispersion parameter (not shown in the output) that is held constant in a Poisson model. Thus, the Poisson model is actually nested in the negative binomial model. We can therefore use a likelihood ratio test to compare these two models and test this model assumption $[1]$.\\

Since $\theta = \frac 1\alpha$, the null hypothesis and alternative hypothesis are:
\[ H_0 : \theta = \infty \qquad vs. \qquad H_1 : \theta < \infty
\]

We use the \Verb+logLik()+ function to obtain the log-likelihood values for the estimated models 2 and 3. We know that the difference between these likelihoods follow a $\chi^2$ distribution with degrees of freedom being equal to the difference in the number of estimated parameters . Then we obtain the p-value for the test statistic.

<<>>=
mod2 <- glm(los ~ type + white + age80 + hmo, family="poisson", data=medpar)
mod3 <- glm.nb(los ~ type + white + age80 + hmo, data=medpar)

print("Poisson Regression Model (Model 2)"); print(logLik(mod2))
print("Negative Binomial Regression Model (Model 3)"); print(logLik(mod3))
print(paste("Test statistic is ",(-2 * (logLik(mod2) - logLik(mod3)))[1]))

pchisq((-2 * (logLik(mod2) - logLik(mod3)))[1], df = 1, lower.tail = FALSE)
@

We see that the test statistics is very large 4256.8940 and comparing it with $\chi^2_1 $ distribution with 1 degree of freedome we obtain the p-value. The p-value is very small (rounded to 0), therefore we can reject null hypothesis. We conclude that model3 models the data better. We can reject a hypothesis that the data are equidisperse.


\begin{thebibliography}{9}

\bibitem{ucla}
UCLA - Statistical Consulting: Negative binomial regression,
\\\texttt{https://stats.idre.ucla.edu/r/dae/negative-binomial-regression/}

\end{thebibliography}

\end{document}
